{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSKHSgObwIUq"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luHpeKqmwNNZ",
        "outputId": "33c71618-6d1a-4ea7-dd75-537c53114d0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gputil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=70d26a30133820a3ad5927c1f2cb95091f12fd21229ee9d268cbd45abe601c95\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (6.0)\n"
          ]
        }
      ],
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "!pip install pyyaml\n",
        "\n",
        "import os\n",
        "import re\n",
        "import gc\n",
        "import sys\n",
        "import time\n",
        "import yaml\n",
        "import errno\n",
        "import hashlib\n",
        "import base64\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from abc import ABC, abstractmethod\n",
        "from IPython.display import display, clear_output\n",
        "from ipywidgets import widgets\n",
        "from tensorflow.test import gpu_device_name\n",
        "from tensorflow.image import random_contrast, random_saturation\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "from tensorflow.keras.layers import Input, Activation, Conv2D, Flatten, MaxPooling2D\n",
        "from tensorflow.keras.layers import SeparableConv2D, concatenate, add, Dense, Dropout\n",
        "from tensorflow.keras.layers import AveragePooling2D, BatchNormalization, ReLU, Add\n",
        "from tensorflow.keras.datasets import cifar10, mnist\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras import utils\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy import optimize\n",
        "from copy import deepcopy\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "ON_COLAB = 'google.colab' in sys.modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWt0VuB_nI4O",
        "outputId": "df168cd5-0b36-49a5-f336-454606cf8571"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "GPU device found!\n",
            "Gen RAM Free: 87.0 GB  | Proc size: 1.3 GB\n",
            "GPU RAM Free: 39934MB | Used: 602MB | Util   1% | Total 40536MB\n",
            "GPU Memory is free!\n"
          ]
        }
      ],
      "source": [
        "''' PRELIMINARY CONFIGURATIONS / TESTS '''\n",
        "\n",
        "def test_colab_GPU_mem():\n",
        "    '''\n",
        "        Colab GPU memory test\n",
        "    '''\n",
        "\n",
        "    import psutil\n",
        "    import humanize\n",
        "    import GPUtil as GPU\n",
        "    GPUs = GPU.getGPUs()\n",
        "\n",
        "    gpu = GPUs[0]\n",
        "\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print('Gen RAM Free: ' + humanize.naturalsize(psutil.virtual_memory().available), ' | Proc size: ' + humanize.naturalsize(process.memory_info().rss))\n",
        "    print('GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB'.format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "    # Check if GPU usage is > 2% (i.e device shared with other Colab users)\n",
        "    print('GPU Memory is shared! Restart the runtime.' if gpu.memoryUtil > 0.05 else 'GPU Memory is free!')\n",
        "\n",
        "\n",
        "if ON_COLAB:\n",
        "    from google.colab import files, drive #, output\n",
        "    # output.enable_custom_widget_manager()\n",
        "    drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "    if gpu_device_name() != '/device:GPU:0':\n",
        "        print('GPU device not found -- Try enabling GPU acceleration in Colab\\'s runtime settings')\n",
        "\n",
        "    else:\n",
        "        print('GPU device found!')\n",
        "        test_colab_GPU_mem()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23ljYpBl6G-p"
      },
      "source": [
        "### Helper Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prompt Handler"
      ],
      "metadata": {
        "id": "txwH2M_YqiTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PromptHandler:\n",
        "    '''\n",
        "        Wrapper for input prompt-handling methods\n",
        "    '''\n",
        "\n",
        "    @staticmethod\n",
        "    def setup_ipy_widgets(on_yaml_export, on_yaml_init):\n",
        "        ''' Sets up configuration loader/exporter IPython widgets '''\n",
        "\n",
        "        path_textarea = widgets.Text(placeholder='/path/to/config.yaml',\n",
        "                                     value='/content/config.yaml')\n",
        "        export_btn = widgets.Button(description=\"Export *Form* Config\")\n",
        "        load_btn = widgets.Button(description=\"Load Config\")\n",
        "        output = widgets.Output()\n",
        "\n",
        "        def on_export_click(b):\n",
        "            path = path_textarea.value\n",
        "\n",
        "            filename = os.path.basename(path)\n",
        "            path = os.path.dirname(path)\n",
        "\n",
        "            with output:\n",
        "                clear_output(wait=True)\n",
        "                if filename == '':\n",
        "                    print('\\nInvalid path / filename!\\n\\n')\n",
        "                on_yaml_export(path, filename)\n",
        "\n",
        "        def on_load_click(b):\n",
        "            path = path_textarea.value\n",
        "\n",
        "            with output:\n",
        "                clear_output(wait=True)\n",
        "                on_yaml_init(path)\n",
        "\n",
        "\n",
        "        export_btn.on_click(on_export_click)\n",
        "        load_btn.on_click(on_load_click)\n",
        "\n",
        "        bot_box = widgets.HBox([load_btn, export_btn])\n",
        "        cont_box = widgets.VBox([path_textarea, bot_box])\n",
        "\n",
        "        display(cont_box, output)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def prompt_yes_no(question, default='y'):\n",
        "        '''\n",
        "            Yes/no query; reverts to default value if no input is given\n",
        "        '''\n",
        "\n",
        "        valid_res = {\n",
        "            'yes': True,\n",
        "            'y': True,\n",
        "            'no': False,\n",
        "            'n': False\n",
        "        }\n",
        "\n",
        "        choice = None\n",
        "\n",
        "        while choice not in valid_res:\n",
        "            choice = input(f'{question} (y/n): ').lower().replace(' ', '') or default\n",
        "\n",
        "        return valid_res[choice]\n",
        "\n"
      ],
      "metadata": {
        "id": "7sKcShxukoBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### File Handler"
      ],
      "metadata": {
        "id": "3HHRRmRhotHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FileHandler:\n",
        "    '''\n",
        "        Wrapper for file-handling methods\n",
        "    '''\n",
        "    \n",
        "    __VALID_PATHS = {}\n",
        "\n",
        "    @staticmethod\n",
        "    def __path_exists(path):\n",
        "        ''' Checks if file exists '''\n",
        "\n",
        "        return os.path.exists(path)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_path(path, ignore_abs_format=True):\n",
        "        ''' \n",
        "            Ensures that a given path is universaly valid\n",
        "            (Windows/Linux/MacOS/POSIX)\n",
        "        '''\n",
        "\n",
        "        # Directory already exists, prompt for overwrite permission (first time only)\n",
        "        if path not in FileHandler.__VALID_PATHS and FileHandler.__path_exists(path):\n",
        "\n",
        "            FileHandler.__VALID_PATHS[path] = True\n",
        "            \n",
        "            if len(os.listdir(path)) == 0:\n",
        "                # directory exists and is empty -> is valid\n",
        "                return FileHandler.__VALID_PATHS[path]\n",
        "\n",
        "            # directory exists and is NOT empty\n",
        "            FileHandler.__VALID_PATHS[path] = True\n",
        "            print(f'\\nPath ({path}) already exists!\\n\\n')\n",
        "            return PromptHandler.prompt_yes_no('Would you like to overwrite files in this path?')\n",
        "\n",
        "        # Previously evaluated\n",
        "        if path in FileHandler.__VALID_PATHS:\n",
        "            return FileHandler.__VALID_PATHS[path]\n",
        "        \n",
        "        # Check the validity of the given path\n",
        "        try:\n",
        "            os.makedirs(path)\n",
        "            FileHandler.__VALID_PATHS[path] = True\n",
        "        except OSError as e:\n",
        "            # path invalid\n",
        "            print('\\nBase path and/or config version are invalid! Please choose path-friendly names.\\n')\n",
        "            FileHandler.__VALID_PATHS[path] = False\n",
        "\n",
        "        return FileHandler.__VALID_PATHS[path]\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def create_dir(path):\n",
        "        ''' Recursively creates new directory if it does not exist '''\n",
        "\n",
        "        if not FileHandler.__path_exists(path):\n",
        "            os.makedirs(path)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def path_must_exist(path):\n",
        "        ''' \n",
        "            Checks if file exists and raises error if it does not.\n",
        "            Used when the logic of the algorithm depends on the loaded file\n",
        "        '''\n",
        "\n",
        "        if not FileHandler.__path_exists(path):\n",
        "            raise FileNotFoundError(errno.ENOENT, \n",
        "                                    os.strerror(errno.ENOENT), \n",
        "                                    path)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def save_pickle(p_dict, path, filename, force_dir=True):\n",
        "        ''' Saves the given dictionary as pickle '''\n",
        "\n",
        "        if not FileHandler.__path_exists(path):\n",
        "            if force_dir:\n",
        "                FileHandler.create_dir(path)\n",
        "            else:\n",
        "                # directory does not exist and cannot create dir\n",
        "                return False\n",
        "        \n",
        "        # dump pickle\n",
        "        with open(os.path.join(path, filename), 'wb') as handle:\n",
        "            pickle.dump(p_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "        return True\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def load_pickle(path, default_dict={}):\n",
        "        ''' Loads pickle and returns decoded dictionary '''\n",
        "\n",
        "        res = default_dict\n",
        "\n",
        "        if FileHandler.__path_exists(path):\n",
        "            with open(path, 'rb') as handle:\n",
        "                res = pickle.load(handle)\n",
        "\n",
        "        return res\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def save_df(df, path, filename, force_dir=True):\n",
        "        ''' Saves the given dataframe to a given path+filename '''\n",
        "\n",
        "        if not FileHandler.__path_exists(path):\n",
        "            if force_dir:\n",
        "                FileHandler.create_dir(path)\n",
        "            else:\n",
        "                # directory does not exist and cannot create dir\n",
        "                return False\n",
        "\n",
        "        # save dataframe\n",
        "        df.to_csv(os.path.join(path, filename))\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def load_df(path, default_df=None):\n",
        "        ''' Loads dataframe if it exists or defaults to an empty df '''\n",
        "\n",
        "        res = default_df or pd.DataFrame()\n",
        "\n",
        "        if FileHandler.__path_exists(path):\n",
        "            res = pd.read_csv(path, header=0, index_col=0)\n",
        "\n",
        "        return res\n",
        "\n",
        "    \n",
        "    @staticmethod\n",
        "    def export_yaml(config_dict, path, filename, file_version_comment='', force_dir=True):\n",
        "        ''' Exports a given dictionary to a yaml file '''\n",
        "        \n",
        "        if not FileHandler.__path_exists(path):\n",
        "            if force_dir:\n",
        "                FileHandler.create_dir(path)\n",
        "            else:\n",
        "                return False\n",
        "            \n",
        "        with open(os.path.join(path, filename), 'w') as handler:\n",
        "            if file_version_comment:\n",
        "                handler.write(f'\\n# {file_version_comment}\\n\\n')\n",
        "            yaml.dump(config_dict, handler, default_flow_style=False)\n",
        "\n",
        "        return True\n",
        "\n",
        "    \n",
        "    @staticmethod\n",
        "    def load_yaml(path, loader, default_dict={}):\n",
        "        ''' Loads a yaml config file and returns it as dict '''\n",
        "\n",
        "        res = default_dict\n",
        "\n",
        "        if FileHandler.__path_exists(path):\n",
        "            with open(path, 'r') as handler:    \n",
        "                res = yaml.load(handler, Loader=loader)\n",
        "\n",
        "        return res\n",
        "\n"
      ],
      "metadata": {
        "id": "t7GSEuUZoyPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pOEN80_BQN1"
      },
      "source": [
        "#### Operational Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaMsCKlzBYyi",
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150,
          "referenced_widgets": [
            "7bd294f16cc648bebd26bd1461b19ebc",
            "3f3bf9039a3246c8aeb20811b1d5a779",
            "d939d647d5b644dc91211aea39fd4242",
            "13ffdd83aca64f7a8a6530f3a6689cf7",
            "b1c84884bbeb4850b4a281bb3d76dd5a",
            "94d00389d9b24553839304d468a355a3",
            "d38c2610c1b14d148f0e1335c2b08657",
            "50a6fdb28783486c94bfde3e55c70d8a",
            "91221d08fb2e421ca2e1182f71e49c7c",
            "2a04366eac8f4d178792418cac6efe59",
            "17aa6bf895d64bc78e7b974f85bdd7ab",
            "23e17a91caf649a2a4ad5c14b3849863",
            "5e3ad6ffab204e839c56c6f520f41872",
            "3896eca9fc86436b913d714df2e90e16",
            "4f8efa4db817469b83dc9ed000360666"
          ]
        },
        "outputId": "efe123d6-7d07-4ecb-e5c1-a7d092e3544b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Text(value='/content/config.yaml', placeholder='/path/to/config.yaml'), HBox(children=(Button(dâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7bd294f16cc648bebd26bd1461b19ebc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3896eca9fc86436b913d714df2e90e16"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "class Params:\n",
        "    '''\n",
        "        Wrapper for all global operational parameters\n",
        "        and the configuration loader\n",
        "    '''\n",
        "\n",
        "    @staticmethod\n",
        "    def config_form():\n",
        "        '''\n",
        "            Facilitates the notebook's form and returns a config dict\n",
        "        '''\n",
        "\n",
        "\n",
        "        ''' Configuration Version (used as filenames) '''\n",
        "        CONFIG_VERSION = 'act_res_aug_cutout_2'   #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "        #@markdown ## ABC Optimizer Parameters\n",
        "        #@markdown ---\n",
        "\n",
        "\n",
        "        ''' Optimization problem (NAS or Numerical Benchmarks to test ABC) '''\n",
        "        OPTIMIZATION_OBJECTIVE = 'HiveNAS'  #@param ['HiveNAS', 'Sphere_max', 'Sphere_min', 'Rosenbrock']\n",
        "\n",
        "        ''' Max trials per Scout (i.e initial Food Source) '''\n",
        "        ABANDONMENT_LIMIT = 5  #@param {type:\"slider\", min:1, max:50, step:1}\n",
        "\n",
        "        ''' Number of bees in the colony (Employees + Onlookers) '''\n",
        "        COLONY_SIZE = 7    #@param {type:\"slider\", min:1, max:50, step:1}\n",
        "\n",
        "        ''' Distribution of Employees to Onlookers, resulting number of EmployeeBees = # of ScoutBees '''\n",
        "        EMPLOYEE_ONLOOKER_RATIO = 0.43   #@param {type:\"slider\", min:0.1, max:1.0, step:0.05}\n",
        "\n",
        "        ''' Number of ABC optimization iterations '''\n",
        "        ITERATIONS_COUNT = 20    #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "\n",
        "\n",
        "        #@markdown \\\n",
        "        #@markdown ## File-Handling Parameters\n",
        "        #@markdown ---\n",
        "\n",
        "\n",
        "        ''' Save results every N evaluations (not iterations) '''\n",
        "        RESULTS_SAVE_FREQUENCY = 1   #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "\n",
        "        '''\n",
        "            Result files base path (path will be created if it does not exist) \n",
        "            A local folder will be created after the CONFIG_VERSION\n",
        "        '''\n",
        "        RESULTS_BASE_PATH = '/content/gdrive/MyDrive/PhD/HiveNAS/'  #@param {type:\"string\"}\n",
        "\n",
        "        ''' Training history files sub-path '''\n",
        "        HISTORY_FILES_SUBPATH = 'training_history/'     #@param {type:\"string\"}\n",
        "\n",
        "        ''' Enable weights saving for resumed training '''\n",
        "        ENABLE_WEIGHT_SAVING = False    #@param {type:\"boolean\"}\n",
        "\n",
        "        ''' Weight files sub-path (ensure that the path exists) '''\n",
        "        WEIGHT_FILES_SUBPATH = 'weights/'    #@param {type:\"string\"}\n",
        "\n",
        "        ''' Specifies whether or not to resume from existing results file (if exists)'''\n",
        "        RESUME_FROM_RESULTS_FILE = False   #@param {type:'boolean'}\n",
        "\n",
        "\n",
        "        #@markdown \\\n",
        "        #@markdown ## NAS Search Space Parameters\n",
        "        #@markdown ---\n",
        "\n",
        "\n",
        "        ''' -- NAS Search Space configuration -- '''\n",
        "\n",
        "        #@markdown *( layers & hyperparameters must be defined as partial functions in code )*\n",
        "        \n",
        "        ''' Number of layers for sampled networks (excludes input/output stems) '''\n",
        "        DEPTH = 5     #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "\n",
        "        ''' Search space operations '''\n",
        "        OPERATIONS = {\n",
        "            'sep5x5_128': partial(SeparableConv2D, filters=128, kernel_size=(5,5), activation='relu', padding='same'),\n",
        "            'sep3x3_128': partial(SeparableConv2D, filters=128, kernel_size=(3,3), activation='relu', padding='same'),\n",
        "            'sep5x5_64': partial(SeparableConv2D, filters=64, kernel_size=(5,5), activation='relu', padding='same'),\n",
        "            'sep3x3_64': partial(SeparableConv2D, filters=64, kernel_size=(3,3), activation='relu', padding='same'),\n",
        "            'sep5x5_32': partial(SeparableConv2D, filters=32, kernel_size=(5,5), activation='relu', padding='same'),\n",
        "            'sep3x3_32': partial(SeparableConv2D, filters=32, kernel_size=(3,3), activation='relu', padding='same'),\n",
        "            'max_pool3x3': partial(MaxPooling2D, pool_size=(3,3), strides=(1,1), padding='same'),\n",
        "            'avg_pool3x3': partial(AveragePooling2D, pool_size=(3,3), strides=(1,1), padding='same'),\n",
        "            'batch_norm': partial(BatchNormalization),\n",
        "            'dropout': partial(Dropout, rate=0.2)\n",
        "        }\n",
        "\n",
        "        ''' '''\n",
        "\n",
        "        # Skip-Connections'/Residual Blocks' occurence rate (0.0 = disabled)\n",
        "        RESIDUAL_BLOCKS_RATE = 0.15    #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "\n",
        "\n",
        "        #@markdown \\\n",
        "        #@markdown ## NAS Evaluation Strategy Parameters\n",
        "        #@markdown ---\n",
        "\n",
        "\n",
        "        ''' -- NAS Evaluation Strategy configuration -- '''\n",
        "\n",
        "        ''' Dataset (classes/inputs are inferred internally) '''\n",
        "        DATASET = 'CIFAR10'   #@param [\"CIFAR10\", \"MNIST\"]\n",
        "\n",
        "        ''' Static output stem, added to every candidate '''\n",
        "        OUTPUT_STEM = [\n",
        "            partial(Flatten),\n",
        "            partial(Dropout, rate=0.15),\n",
        "            partial(Dense, units=1024, activation='relu'),\n",
        "            partial(Dropout, rate=0.15),\n",
        "            partial(Dense, units=512, activation='relu')\n",
        "        ]\n",
        "\n",
        "        ''' Static input stem, added to every candidate '''\n",
        "        INPUT_STEM = [\n",
        "            partial(Conv2D, filters=32, kernel_size=(3,3)),\n",
        "            partial(BatchNormalization),\n",
        "            partial(ReLU)\n",
        "        ]\n",
        "\n",
        "        ''' Epochs count per candidate network '''\n",
        "        EPOCHS = 3  #@param {type:\"slider\", min:1, max:25, step:1}\n",
        "        \n",
        "        ''' Momentum Augmentation epochs (0 = disabled ; overrides ENABLE_WEIGHT_SAVING) '''\n",
        "        MOMENTUM_EPOCHS = 10 #@param {type:\"slider\", min:0, max:25}\n",
        "\n",
        "        ''' Epochs count for the best performing candidate upon full training '''\n",
        "        FULL_TRAIN_EPOCHS = 50 #@param {type:\"slider\", min:1, max:150, step:1}\n",
        "\n",
        "        ''' \n",
        "            Threshold factor (beta) for early-stopping (refer to the TerminateOnThreshold class for details)\n",
        "                1.0 = all networks will be terminated (minimum accuracy = 100%)\n",
        "                0.0 = disable early-stopping, all networks will pass\n",
        "                0.25 = for 10 classes, val_acc > 0.325 at epoch 1 will not be terminated\n",
        "                       (tolerance decreased for every subsequent epoch)\n",
        "        '''\n",
        "        TERMINATION_THRESHOLD_FACTOR = 0.25 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "\n",
        "        ''' Diminishing factor (zeta) for termination threshold over epochs '''\n",
        "        TERMINATION_DIMINISHING_FACTOR = 0.25 #@param {type:\"slider\", min:0.1, max:1.0, step:0.05}\n",
        "\n",
        "        ''' Learning rate (overrides default optimizer lr) '''\n",
        "        LR = 0.001  #@param {type:\"slider\", min:0.001, max:0.1, step:0.001}\n",
        "\n",
        "        ''' Batch size for every candidate evaluation '''\n",
        "        BATCH_SIZE = 128     #@param {type:\"slider\", min:8, max:256, step:2}\n",
        "\n",
        "        ''' Optimizer used for both NAS and full-training methods '''\n",
        "        OPTIMIZER = 'Adam'    #@param [\"Adam\", \"RMSprop\"]\n",
        "\n",
        "\n",
        "        #@markdown \\\n",
        "        #@markdown ## Data Augmentation Parameters\n",
        "        #@markdown ---\n",
        "\n",
        "\n",
        "        ''' \n",
        "            Enable affine transformations augmentation \n",
        "            (horizontal/vertical shifts, rotation, etc...)\n",
        "        '''\n",
        "\n",
        "        AFFINE_TRANSFORMATIONS_ENABLED = True   #@param {type:\"boolean\"}\n",
        "\n",
        "        ''' Probability of random cutout augmentation occurence (0.0 = disabled) '''\n",
        "        CUTOUT_PROB = 0.5    #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "\n",
        "        ''' Probability of random saturation augmentation occurence (0.0 = disabled) '''\n",
        "        SATURATION_AUG_PROB = 0.5    #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "\n",
        "        ''' Probability of random contrast augmentation occurence (0.0 = disabled) '''\n",
        "        CONTRAST_AUG_PROB = 0.5    #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "\n",
        "        return locals()\n",
        "\n",
        "\n",
        "    ''' Main configuration dict '''\n",
        "    __CONFIG = config_form.__func__()\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def init_from_yaml(path):\n",
        "        ''' Initializes the global parameters from a given yaml config file '''\n",
        "\n",
        "        def param_op_constructor(loader: yaml.SafeLoader, node: yaml.nodes.MappingNode):\n",
        "            # constructs an operation partial function from yaml !Operation tags\n",
        "            op_dict = loader.construct_mapping(node)\n",
        "            op = op_dict['op']\n",
        "            del op_dict['op']\n",
        "\n",
        "            return partial(globals()[op], **op_dict)\n",
        "            \n",
        " \n",
        "        def param_tuple_constructor(loader: yaml.SafeLoader, node: yaml.nodes.MappingNode):\n",
        "            # because for some reason we need an explicit tuple constructor\n",
        "\n",
        "            return tuple(loader.construct_sequence(node))\n",
        "\n",
        "        # register constructors\n",
        "        loader = yaml.SafeLoader\n",
        "        loader.add_constructor(u'tag:yaml.org,2002:python/tuple', param_tuple_constructor)\n",
        "        loader.add_constructor('!Operation', param_op_constructor)\n",
        "\n",
        "        config = FileHandler.load_yaml(path, loader)\n",
        "\n",
        "        if not config:\n",
        "            print(f'\\nConfig file ({path}) is either invalid or does not exist.\\n\\n')\n",
        "            return\n",
        "\n",
        "        for key,val in config.items():\n",
        "            Params.__CONFIG[key] = val\n",
        "\n",
        "        print(f'\\nSuccessfully loaded the operational parameters from {path}.\\n\\n')\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def export_yaml(path, filename, from_formdata=False):\n",
        "        ''' Saves the current configurations to the given path as yaml '''\n",
        "\n",
        "        def param_op_representer(dumper, data):\n",
        "            # serialize partial functions into yaml !Operation\n",
        "            serialized_data = {'op': data.func.__name__}\n",
        "            serialized_data.update(data.keywords)\n",
        "            \n",
        "            return dumper.represent_mapping('!Operation', serialized_data, flow_style=True)\n",
        "\n",
        "        def param_tuple_representer(dumper, data):\n",
        "            # serialize tuples into yaml !!python/tuple\n",
        "\n",
        "            return dumper.represent_sequence(u'tag:yaml.org,2002:python/tuple', data, flow_style=True)\n",
        "\n",
        "        # register representers\n",
        "        yaml.add_representer(tuple, param_tuple_representer)\n",
        "        yaml.add_representer(partial, param_op_representer)\n",
        "        yaml.Dumper.ignore_aliases = lambda *args : True\n",
        "\n",
        "        # data source (changing the Colab form does not reflect on the main dict)\n",
        "        data = Params.config_form() if from_formdata else Params.__CONFIG\n",
        "\n",
        "        if FileHandler.export_yaml(Params.config_form(),\n",
        "                                   path,\n",
        "                                   filename):\n",
        "            print(f'\\nConfiguration file saved successfully to ({os.path.join(path, filename)})!\\n\\n')\n",
        "        else:\n",
        "            print('\\nFailed to save config file!\\n\\n')\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def search_space_config():\n",
        "        '''  Returns the search space config dict '''\n",
        "\n",
        "        res = {\n",
        "            'depth': Params['DEPTH'],\n",
        "            'operations': Params['OPERATIONS'],\n",
        "            'residual_blocks_rate': Params['RESIDUAL_BLOCKS_RATE']\n",
        "        }\n",
        "\n",
        "        return res\n",
        "    \n",
        "\n",
        "    @staticmethod\n",
        "    def evaluation_strategy_config():\n",
        "        '''  Returns the evaluation strategy config dict '''\n",
        "\n",
        "        res = {\n",
        "            'dataset': Params['DATASET'],\n",
        "            'operations': Params['OPERATIONS'],\n",
        "            'output_stem': Params['OUTPUT_STEM'],\n",
        "            'input_stem': Params['INPUT_STEM'],\n",
        "            'epochs': Params['EPOCHS'],\n",
        "            'full_train_epochs': Params['FULL_TRAIN_EPOCHS'],\n",
        "            'lr': Params['LR'],\n",
        "            'batch_size': Params['BATCH_SIZE'],\n",
        "            'optimizer': globals()[Params['OPTIMIZER']],\n",
        "            'termination_threshold_factor': Params['TERMINATION_THRESHOLD_FACTOR'],\n",
        "            'termination_diminishing_factor': Params['TERMINATION_DIMINISHING_FACTOR'],\n",
        "            'momentum_epochs': Params['MOMENTUM_EPOCHS']\n",
        "        }\n",
        "\n",
        "        return res\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def get_results_path():\n",
        "        '''  Gets the results path from RESULTS_BASE_PATH and CONFIG_VERSION '''\n",
        "\n",
        "        path = os.path.join(Params.__CONFIG['RESULTS_BASE_PATH'],\n",
        "                            f'{Params.__CONFIG[\"CONFIG_VERSION\"]}/')\n",
        "\n",
        "        if FileHandler.validate_path(path):\n",
        "            return path\n",
        "\n",
        "        return None\n",
        "\n",
        "\n",
        "    def __class_getitem__(cls, key):\n",
        "        ''' Subscript operator definition '''\n",
        "            \n",
        "        return Params.__CONFIG[key]\n",
        "\n",
        "\n",
        "''' Init widgets if ran on Colab '''\n",
        "if ON_COLAB:\n",
        "    PromptHandler.setup_ipy_widgets(on_yaml_export=lambda p, f: Params.export_yaml(p, f, True),\n",
        "                                    on_yaml_init=lambda p: Params.init_from_yaml(p))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-DtNe9hv4p0"
      },
      "source": [
        "#### Logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRTp4FdmwD3l"
      },
      "outputs": [],
      "source": [
        "class Logger:\n",
        "    '''\n",
        "        Wrapper for debug- and info-logging methods\n",
        "    '''\n",
        "\n",
        "    __DEBUG_PREFIX  = 'DEBUG:'\n",
        "    __STATUS_PREFIX = 'STATUS:'\n",
        "    __EVAL_PREFIX = 'EVALUATION LOG:'\n",
        "    __FILESAVE_PREFIX = 'FILE-SAVE SUCCESSFUL:'\n",
        "\n",
        "    __START_TIME = None\n",
        "\n",
        "    EVALUATION_LOGGING = False\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def debug(msg=None):\n",
        "        ''' Debugging messages '''\n",
        "\n",
        "        print('{} {}'.format(Logger.__DEBUG_PREFIX,\n",
        "                            ('MARK' if msg is None else str(msg))))\n",
        "\n",
        "    \n",
        "    @staticmethod\n",
        "    def status(itr, msg=None):\n",
        "        ''' Generic logging '''\n",
        "\n",
        "        print('{} itr: {} -- {}'.format(Logger.__STATUS_PREFIX,\n",
        "                                        str(itr),\n",
        "                                        ('MARK' if msg is None else str(msg))))\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def evaluation_log(type, id, candidate_pos):\n",
        "        ''' Logs pre-evaluation info for every candidate '''\n",
        "\n",
        "        if not Logger.EVALUATION_LOGGING:\n",
        "            return\n",
        "\n",
        "        print('\\n{} {} ID ({}) -- Candidate ({})\\n'.format(Logger.__EVAL_PREFIX,\n",
        "                                                           type,\n",
        "                                                           str(id),\n",
        "                                                           str(candidate_pos)))\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def filesave_log(candidate, filename):\n",
        "        ''' Logs candidate info upon file-save '''\n",
        "\n",
        "        if not Logger.EVALUATION_LOGGING:\n",
        "            return\n",
        "\n",
        "        print('\\n{} Candidate ({}) was saved to {}\\n'.format(Logger.__FILESAVE_PREFIX,\n",
        "                                                             str(candidate),\n",
        "                                                             filename))\n",
        "        \n",
        "\n",
        "    @staticmethod\n",
        "    def start_log():\n",
        "        ''' Logs the start msg and intializes the global timer '''\n",
        "\n",
        "        Logger.__START_TIME = time.time()\n",
        "        dashes = '------------------------'\n",
        "        print('{}\\n-- OPTIMIZATION START --\\n{}'.format(dashes, dashes))\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def end_log():\n",
        "        ''' Logs total time taken upon optimization end '''\n",
        "\n",
        "        end_time = time.time() - Logger.__START_TIME\n",
        "        dashes = '---------------------'\n",
        "        print('{}\\n-- OPTIMIZATION END --\\n{} === TOTAL TIME TAKEN: {} ==== \\n'.format(dashes, dashes, end_time))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Image Augmentations"
      ],
      "metadata": {
        "id": "Lb79g_x8MmGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImgAug:\n",
        "    ''' \n",
        "        Element-wise image augmentation methods, used to preprocess a\n",
        "        given dataset.\n",
        "        (most affine transformations are already implemented in \n",
        "        Keras' ImageDataGenerator)\n",
        "    '''\n",
        "\n",
        "    @staticmethod\n",
        "    def random_cutout(np_tensor):\n",
        "        '''\n",
        "            Randomly applies cutout augmentation to a given rank 3 tensor as\n",
        "            defined in [1].\n",
        "\n",
        "            [1] DeVries, T., & Taylor, G. W. (2017). Improved regularization of \n",
        "            convolutional neural networks with cutout.\n",
        "        '''\n",
        "\n",
        "        cutout_height = int(np.random.uniform(0.1, 0.2) * np_tensor.shape[0])\n",
        "        cutout_width = int(np.random.uniform(0.1, 0.2) * np_tensor.shape[1])\n",
        "\n",
        "        cutout_height_point = np.random.randint(np_tensor.shape[0] - cutout_height)\n",
        "        cutout_width_point = np.random.randint(np_tensor.shape[1] - cutout_width)\n",
        "\n",
        "        np_tensor[cutout_height_point: cutout_height_point + cutout_height, \n",
        "                  cutout_width_point: cutout_width_point + cutout_width, \n",
        "                  :] = 127    # 127 = grey cutout,\n",
        "                              # 0 (black) or 255 (white) also valid\n",
        "        \n",
        "        return np.array(np_tensor)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def random_contrast(np_tensor):\n",
        "        ''' Apply random contrast augmentation '''\n",
        "        \n",
        "        return np.array(random_contrast(np_tensor, 0.5, 2))\n",
        "\n",
        "    @staticmethod\n",
        "    def random_saturation(np_tensor):\n",
        "        ''' Apply random saturation augmentation '''\n",
        "\n",
        "        return np.array(random_saturation(np_tensor, 0.2, 3))\n",
        "\n",
        "    @staticmethod\n",
        "    def augment(np_tensor):\n",
        "        ''' Used by ImageDataGenerator's preprocess_function '''\n",
        "\n",
        "        if np.random.uniform() <= Params['CONTRAST_AUG_PROB']:\n",
        "            np_tensor = ImgAug.random_contrast(np_tensor)\n",
        "        if np.random.uniform() <= Params['SATURATION_AUG_PROB']:\n",
        "            np_tensor = ImgAug.random_saturation(np_tensor)\n",
        "        if np.random.uniform() <= Params['CUTOUT_PROB']:\n",
        "            np_tensor = ImgAug.random_cutout(np_tensor)\n",
        "\n",
        "        return np_tensor\n",
        "\n"
      ],
      "metadata": {
        "id": "Ta31nP_TM-nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC7Um7Lb5xxE"
      },
      "source": [
        "### NAS Components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3Isw0Wo2L9_"
      },
      "source": [
        "#### NAS Search Space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuFJeLRt1-i_"
      },
      "outputs": [],
      "source": [
        "class NASSearchSpace(object):\n",
        "    ''' Defines the Search Space used to sample candidates by HiveNAS '''\n",
        "         \n",
        "    def __init__(self, config):\n",
        "        '''\n",
        "            Configurations are predefined in the Params class,\n",
        "            the implementation should work given any set of operations' mapping\n",
        "            and depth\n",
        "        '''\n",
        "\n",
        "        self.config = config\n",
        "        # self.__initialize_graph()\n",
        "                    \n",
        "\n",
        "    def sample(self):\n",
        "        '''\n",
        "            Samples a random point in the search space\n",
        "        '''\n",
        "\n",
        "        # assert self.all_paths != None, 'Search space needs to be initialized!'\n",
        "\n",
        "        # idx = np.random.randint(0, len(self.all_paths))\n",
        "        # return self.__encode_path(self.all_paths[idx])\n",
        "\n",
        "        path = ['input']\n",
        "\n",
        "        for l in range(self.config['depth']):\n",
        "\n",
        "            if np.random.rand() < self.config['residual_blocks_rate']:\n",
        "                sc_depth = np.random.randint(1, self.config['depth'] - l + 1)\n",
        "                path.append('L{}_sc_{}'.format(l+1, sc_depth))\n",
        "\n",
        "            path.append('L{}_{}'.format(l+1, np.random.choice(\n",
        "                list(self.config['operations'].keys())\n",
        "            )))\n",
        "        \n",
        "        path.append('output')\n",
        "\n",
        "        return self.__encode_path(path)\n",
        "\n",
        "\n",
        "    def get_neighbor(self, path_str):\n",
        "        ''' Returns a path with 1-op difference (a neighbor)'''\n",
        "\n",
        "        path = self.__strip_path(self.__decode_path(path_str))\n",
        "\n",
        "        component = np.random.randint(1, len(path) - 1)\n",
        "\n",
        "        ops = []\n",
        "        if path[component].startswith('sc'):\n",
        "            # modify skip-connection (either remove it or change residual depth)\n",
        "            sc_max_depth = len([op for op in path[component:] if not op.startswith('sc')])\n",
        "            ops = [f'sc_{i}' for i in range(sc_max_depth)]\n",
        "            ops.remove(path[component])\n",
        "        else:\n",
        "            # modify operation\n",
        "            ops = list(self.config['operations'].keys())\n",
        "            ops.remove(path[component])\n",
        "        \n",
        "        # Replace randomly chosen component (operation) with any other op\n",
        "        path[component] = np.random.choice(ops)\n",
        "\n",
        "        # prune skip-connection if op == sc_0\n",
        "        if path[component] == 'sc_0':\n",
        "            del path[component]\n",
        "\n",
        "        return self.__encode_path(path)\n",
        "\n",
        "\n",
        "    def eval_format(self, path):\n",
        "        ''' \n",
        "            Formats a path for evaluation (stripped, decoded, and\n",
        "            excluding input/output layers) given a string-encoded path\n",
        "        '''\n",
        "\n",
        "        return self.__strip_path(self.__decode_path(path))[1:-1]\n",
        "\n",
        "\n",
        "    def __initialize_graph(self):\n",
        "        '''\n",
        "            Initializes the search space DAG for easier sampling by the\n",
        "            search algorithm\n",
        "            [Deprecated] -- The search space DAG-representation is too memory-expensive\n",
        "        '''\n",
        "        \n",
        "        self.dag = nx.DiGraph()\n",
        "        self.dag.add_node('input')\n",
        "\n",
        "        for l in range(self.config['depth']):\n",
        "            for op in self.config['operations']:\n",
        "                # Connect input layer to first hidden layer\n",
        "                if l == 0:\n",
        "                    self.dag.add_edges_from([('input', \n",
        "                                              'L{}_{}'.format(l+1, op))])\n",
        "                    continue\n",
        "\n",
        "                # Densely connect middle layers\n",
        "                for prev_op in self.config['operations']:\n",
        "                    self.dag.add_edges_from([('L{}_{}'.format(l, prev_op), \n",
        "                                              'L{}_{}'.format(l+1, op))])\n",
        "\n",
        "                # Connect last hidden layer to output stem\n",
        "                if l == self.config['depth'] - 1:\n",
        "                    self.dag.add_edges_from([('L{}_{}'.format(l+1, op), \n",
        "                                              'output')])\n",
        "\n",
        "        self.all_paths = list(nx.all_simple_paths(self.dag, 'input', 'output'))\n",
        "\n",
        "\n",
        "    def __encode_path(self, path):\n",
        "        ''' Returns a string encoding of a given path (list of ops)'''\n",
        "\n",
        "        return '|'.join(self.__strip_path(path))\n",
        "\n",
        "\n",
        "    def __decode_path(self, path):\n",
        "        ''' Returns a list of ops given a string-encoded path '''\n",
        "\n",
        "        ops = path.split('|')\n",
        "\n",
        "        for i in range(1, len(ops) - 1):\n",
        "            ops[i] = 'L{}_{}'.format(i, ops[i])\n",
        "\n",
        "        return ops\n",
        "\n",
        "\n",
        "    def __strip_path(self, path):\n",
        "        ''' Strips path of layer ID prefixes given a list of ops '''\n",
        "        \n",
        "        return [re.sub('L\\d+_', '', s) for s in path]\n",
        "\n",
        "\n",
        "    def compute_space_size(self):\n",
        "        ''' \n",
        "            Returns the number of possible architectures in the given space\n",
        "            (i.e operations and depth) for analytical purposes\n",
        "        '''\n",
        "\n",
        "        return len(list(self.config['operations'].keys())) ** \\\n",
        "        self.config['depth']\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzZBH0Zu5AUq"
      },
      "source": [
        "#### NAS Evaluation Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IidYdvn05Djp"
      },
      "outputs": [],
      "source": [
        "class NASEval(object):\n",
        "    ''' Responsible for instantiating and evaluating candidate architectures '''\n",
        "\n",
        "    def __init__(self, config):\n",
        "        ''' \n",
        "            Initializes the evaluation parameters' configuration ;\n",
        "            for a different dataset, a data-loader must be specified below\n",
        "            as with CIFAR10 Keras loader\n",
        "        '''\n",
        "        \n",
        "        self.config = config\n",
        "\n",
        "        # specify dataset loaders\n",
        "        if config['dataset'] == 'CIFAR10':\n",
        "            (self.X_train, self.y_train), (self.X_test, self.y_test) = cifar10.load_data()\n",
        "        elif config['dataset'] == 'MNIST':\n",
        "            (self.X_train, self.y_train), (self.X_test, self.y_test) = mnist.load_data()\n",
        "            # Add a placeholder dimension to the dataset to match RGB image datasets\n",
        "            self.X_train = self.X_train.reshape(-1,28,28,1)\n",
        "            self.X_test = self.X_test.reshape(-1,28,28,1) \n",
        "        elif config['dataset'] == 'IMAGE_NET':\n",
        "            pass\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "        if Params['ENABLE_WEIGHT_SAVING']:\n",
        "            # create directory if it does not exist\n",
        "            FileHandler.create_dir(os.path.join(Params.get_results_path(),\n",
        "                                                Params['WEIGHT_FILES_SUBPATH']))\n",
        "\n",
        "        self.__initialize_dataset()\n",
        "\n",
        "\n",
        "    def __instantiate_network(self, arch):\n",
        "        ''' Instantiates a Keras network given an architecture op list '''\n",
        "\n",
        "        # residual counters\n",
        "        res_count = []\n",
        "\n",
        "        # add input according to given dataset shape\n",
        "        net = inputs = Input(shape=(self.X_train.shape[1:4]))\n",
        "\n",
        "        # add input stem\n",
        "        for op in self.config['input_stem']:\n",
        "            net = op()(net)\n",
        "\n",
        "        # add hidden layers\n",
        "        for layer in arch:\n",
        "\n",
        "            if layer.startswith('sc'):\n",
        "                # start residual block\n",
        "                res_count.append((net, int(layer[3:])))\n",
        "                continue\n",
        "\n",
        "            assert layer in self.config['operations'], 'Operation must be defined as a partial in HIVE_EVAL_CONFIG'\n",
        "            net = self.config['operations'][layer]()(net)\n",
        "\n",
        "            for idx, row in enumerate(res_count):\n",
        "                connection, counter = row\n",
        "                counter -= 1\n",
        "\n",
        "                # apply pooling to residual blocks to maintain shape\n",
        "                # [deprecated] -- pooling layers padded\n",
        "                # if 'pool' in layer:\n",
        "                #     connection = self.config['operations'][layer]()(connection)\n",
        "\n",
        "                if counter == 0:\n",
        "                    # conv1x1 to normalize channels\n",
        "                    fx = Conv2D(net.shape[-1], (1, 1), padding='same')(connection)\n",
        "                    net = Add()([fx, net])\n",
        "                    del res_count[idx]\n",
        "                else:\n",
        "                    res_count[idx] = (connection, counter)\n",
        "\n",
        "        # add output stem\n",
        "        for op in self.config['output_stem']:\n",
        "            net = op()(net)\n",
        "\n",
        "        # add output layer\n",
        "        net = Dense(len(np.unique(self.y_test)), activation='softmax')(net)\n",
        "\n",
        "        self.model = Model(inputs, net)\n",
        "\n",
        "\n",
        "    def get_weights_filename(self, arch):\n",
        "        ''' \n",
        "            Hashes the architecture op-list into a filename\n",
        "        '''\n",
        "\n",
        "        return hashlib.sha1(''.join(arch).encode(\"UTF-8\")).hexdigest()\n",
        "\n",
        "\n",
        "    def evaluate(self, arch):\n",
        "        '''\n",
        "            Evaluates the candidate architecture given a string-encoded\n",
        "            representation of the model\n",
        "        '''\n",
        "\n",
        "        # instantiate/compile model\n",
        "        self.__instantiate_network(arch)\n",
        "        self.__compile_model()\n",
        "        \n",
        "        # train model\n",
        "        # self.model.fit(x=self.X_train,\n",
        "        #         y=self.y_train,\n",
        "        #         batch_size=self.config['batch_size'],\n",
        "        #         epochs=self.config['epochs'],\n",
        "        #         verbose=1,\n",
        "        #         validation_data=(self.X_test, self.y_test))\n",
        "        \n",
        "        assert self.config['epochs'] > 2 or self.config['momentum_epochs'] == 0, 'Momentum Augmentation requires at least 3 epochs per candidate'\n",
        "\n",
        "        cb = []\n",
        "        if self.config['termination_threshold_factor'] > 0.0:\n",
        "            cb.append(TerminateOnThreshold(threshold_multiplier=self.config['termination_threshold_factor'],\n",
        "                                           diminishing_factor=self.config['termination_diminishing_factor']))\n",
        "        if self.config['momentum_epochs'] > 0:\n",
        "            cb.append(MomentumAugmentation())\n",
        "\n",
        "        history = self.model.fit(self.datagen.flow(self.X_train,\n",
        "                                                   self.y_train,\n",
        "                                                   shuffle=True,\n",
        "                                                   batch_size=self.config['batch_size'],\n",
        "                                                   subset='training'),\n",
        "                                 validation_data=self.datagen.flow(self.X_train,\n",
        "                                                                   self.y_train,\n",
        "                                                                   batch_size=int(self.config['batch_size'] / 2), \n",
        "                                                                   subset='validation'),\n",
        "                                 epochs=self.config['epochs'],\n",
        "                                 verbose=1,\n",
        "                                 callbacks=cb)\n",
        "\n",
        "        momentum = self.model.momentum if hasattr(self.model, 'momentum') else {}\n",
        "        # test model\n",
        "        eval_res = self.model.evaluate(self.X_test,\n",
        "                                      self.y_test,\n",
        "                                      batch_size=self.config['batch_size'],\n",
        "                                      verbose=1)\n",
        "        \n",
        "        # dump training history\n",
        "        hist_path = os.path.join(Params.get_results_path(), Params['HISTORY_FILES_SUBPATH'])\n",
        "        hist_fn = self.get_weights_filename(arch) + '.pickle'\n",
        "\n",
        "        FileHandler.save_pickle(history.history, hist_path, hist_fn)\n",
        "        \n",
        "        # save weights for later retraining when needed\n",
        "        filename = self.get_weights_filename(arch) + '.h5'\n",
        "\n",
        "        if Params['ENABLE_WEIGHT_SAVING']:\n",
        "            model_path = os.path.join(Params.get_results_path(), Params['WEIGHT_FILES_SUBPATH'])\n",
        "            self.model.save(model_path + filename)\n",
        "\n",
        "        trainable_params = np.sum([K.count_params(w) for w in self.model.trainable_weights])\n",
        "\n",
        "        # housekeeping\n",
        "        del self.model\n",
        "        \n",
        "        # return validation accuracy to maximize + additional data for saving purposes\n",
        "        retval = {\n",
        "            'fitness': eval_res[1], \n",
        "            'epochs': len(history.history['loss']),\n",
        "            'filename': filename,\n",
        "            'params': trainable_params,\n",
        "            'momentum': momentum\n",
        "        }\n",
        "\n",
        "        return retval\n",
        "\n",
        "    \n",
        "    def fully_train(self, model_file=None, arch=None):\n",
        "        ''' Loads and continues training of a partially-trained model '''\n",
        "\n",
        "        hist_path = os.path.join(Params.get_results_path(), Params['HISTORY_FILES_SUBPATH'])\n",
        "        hist_fn = ''\n",
        "\n",
        "        if model_file is not None:\n",
        "            # load model\n",
        "            self.model = tf.keras.models.load_model(model_file)\n",
        "            hist_fn =  model_file.split('/')[-1] + '.full.pickle'\n",
        "        else:\n",
        "            # instantiate network\n",
        "            self.__instantiate_network(arch)\n",
        "            self.__compile_model()\n",
        "            model_file = self.get_weights_filename(arch)\n",
        "            hist_fn = model_file + '.full.pickle'\n",
        "        \n",
        "        # continue training\n",
        "        # self.model.fit(x=self.X_train,\n",
        "        #                y=self.y_train,\n",
        "        #                batch_size=self.config['batch_size'],\n",
        "        #                epochs=self.config['full_train_epochs'],\n",
        "        #                verbose=1,\n",
        "        #                validation_data=(self.X_test, self.y_test))\n",
        "        \n",
        "        history = self.model.fit(self.datagen.flow(self.X_train, \n",
        "                                                   self.y_train,\n",
        "                                                   shuffle=True,\n",
        "                                                   batch_size=self.config['batch_size'],\n",
        "                                                   subset='training'),\n",
        "                                 validation_data=self.datagen.flow(self.X_train,\n",
        "                                                                   self.y_train,\n",
        "                                                                   batch_size=int(self.config['batch_size'] / 2), \n",
        "                                                                   subset='validation'),\n",
        "                                  epochs=self.config['full_train_epochs'],\n",
        "                                  verbose=1)\n",
        "\n",
        "        \n",
        "        # test model\n",
        "        eval_res = self.model.evaluate(self.X_test,\n",
        "                                       self.y_test,\n",
        "                                       batch_size=self.config['batch_size'],\n",
        "                                       verbose=1)\n",
        "        \n",
        "        # dump training history\n",
        "        FileHandler.save_pickle(history.history, hist_path, hist_fn)\n",
        "        \n",
        "        # save weights for later retraining when needed\n",
        "        # if Params['ENABLE_WEIGHT_SAVING']: \n",
        "        # Save fully trained model regardless of params\n",
        "        self.model.save(model_file + '.full.h5')\n",
        "\n",
        "        trainable_params = np.sum([K.count_params(w) for w in self.model.trainable_weights])\n",
        "\n",
        "        # housekeeping\n",
        "        del self.model\n",
        "        \n",
        "        retval = {\n",
        "            'fitness': eval_res[1], \n",
        "            'epochs': len(history.history['loss']),\n",
        "            'filename': model_file,\n",
        "            'params': trainable_params\n",
        "        }\n",
        "\n",
        "        return retval\n",
        "\n",
        "    \n",
        "    def __initialize_dataset(self):\n",
        "        ''' \n",
        "            Prepares the dataset (Normalization -> One-Hot Encoding)\n",
        "            and initializes the ImageGenerator for evaluation\n",
        "        '''\n",
        "\n",
        "        # Standardize data\n",
        "        X_train_mean = np.mean(self.X_train, axis=(0,1,2))\n",
        "        X_train_std = np.std(self.X_train, axis=(0,1,2))\n",
        "        self.X_train = (self.X_train - X_train_mean) / X_train_std\n",
        "        self.X_test = (self.X_test - X_train_mean) / X_train_std\n",
        "\n",
        "        # Affine transformations\n",
        "        if Params['AFFINE_TRANSFORMATIONS_ENABLED']:\n",
        "            self.datagen = ImageDataGenerator(\n",
        "                zoom_range = [0.8, 1.1], \n",
        "                shear_range= 10,\n",
        "                rotation_range=15,\n",
        "                width_shift_range=0.1,\n",
        "                height_shift_range=0.1,\n",
        "                horizontal_flip=True,\n",
        "                preprocessing_function=ImgAug.augment,\n",
        "                validation_split=0.2\n",
        "            )\n",
        "        else:\n",
        "            self.datagen = ImageDataGenerator(preprocessing_function=ImgAug.augment,\n",
        "                                              validation_split=0.2)\n",
        "\n",
        "        # per docs, .fit() is only needed if the generator enables:\n",
        "        # featurewise_center or featurewise_std_normalization or zca_whitening\n",
        "        # self.datagen.fit(self.X_train)\n",
        "\n",
        "        # # One-hot encoding\n",
        "        # deprecated; memory consumption too high for intermediate tensors\n",
        "        # self.y_train = utils.to_categorical(self.y_train)\n",
        "        # self.y_test = utils.to_categorical(self.y_test)\n",
        "\n",
        "\n",
        "    def __compile_model(self):\n",
        "        ''' Compiles model in preparation for evaluation '''\n",
        "\n",
        "        self.model.compile(loss='sparse_categorical_crossentropy', \\\n",
        "                           optimizer=self.config['optimizer'](), \\\n",
        "                           metrics=['sparse_categorical_accuracy'])\n",
        "        \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Adaptive Cutoff Threshold"
      ],
      "metadata": {
        "id": "pAoHETeqfoDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TerminateOnThreshold(Callback):\n",
        "    \"\"\"\n",
        "        Adaptive Cutoff Threshold (ACT)\n",
        "\n",
        "        Keras Callback that terminates training if a given 'sparse_categorical_accuracy'\n",
        "        dynamic threshold is not reached after n_epochs.\n",
        "        The termination threshold has a logarithmic nature where the threshold\n",
        "        increases by a decaying factor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, \n",
        "                monitor='val_sparse_categorical_accuracy', \n",
        "                threshold_multiplier=0.25,\n",
        "                diminishing_factor=0.25,\n",
        "                n_classes = None):\n",
        "        ''' Initialize threshold-based termination callback '''\n",
        "        \n",
        "        super(TerminateOnThreshold, self).__init__()\n",
        "\n",
        "        self.monitor = monitor\n",
        "        self.beta = threshold_multiplier\n",
        "        self.zeta = diminishing_factor\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    def get_threshold(self, epoch):\n",
        "        ''' Calculates val_acc termination threshold given the current epoch '''\n",
        "        '''\n",
        "            Î”Threshold = ÃŸ(1 - (1 / n))\n",
        "            Threshold_base = (1 / n) + Î”Threshold = (1 / n) + ÃŸ(1 - (1 / n))\n",
        "                                                  = (1 + ÃŸn - ÃŸ) / n\n",
        "\n",
        "            Range of Threshold_base = (1 / n, 1) ; horizontal asymptote at 1\n",
        "            Î”Threshold decays as the number of classes decreases\n",
        "            \n",
        "            --------------\n",
        "\n",
        "            To account for the expected increase in accuracy over the number\n",
        "            of epochs Îµ, a growth_factor is added to the base threshold:\n",
        "\n",
        "            growth_factor = (1 - Threshold_base) - (1 / (1 / 1-Threshold_base) + Î¶(Îµ - 1))\n",
        "            \n",
        "            Threshold_adaptive = Threshold_base + growth_factor\n",
        "\n",
        "            Range of growth_factor = [Threshold_base, 1) ; horizontal asymptote at 1\n",
        "        '''\n",
        "\n",
        "        baseline = 1.0 / self.n_classes     # baseline (random) val_acc\n",
        "        complement_baseline = 1 - baseline\n",
        "        delta_threshold = complement_baseline * self.beta\n",
        "        base_threshold = baseline + delta_threshold\n",
        "        ''' n_classes = 10, threshold_multiplier = 0.15 '''\n",
        "        ''' yields .325 acc threshold for epoch 1 '''\n",
        "\n",
        "        # epoch-based decaying increase in val_acc threshold\n",
        "        complement_threshold = 1 - base_threshold    # the increase factor's upper limit\n",
        "        growth_denom = (1.0 / complement_threshold) + self.zeta * (epoch - 1)\n",
        "        growth_factor = complement_threshold - 1.0 / growth_denom\n",
        "\n",
        "        calculated_threshold = base_threshold + growth_factor\n",
        "        ''' \n",
        "            Same settings as before yields:\n",
        "            epoch 1 = .325000\n",
        "            epoch 2 = .422459, \n",
        "            epoch 3 = .495327,\n",
        "            epoch 4 = .551867,\n",
        "            epoch 5 = .597014\n",
        "        '''\n",
        "        \n",
        "        return calculated_threshold\n",
        "\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        ''' Called by Keras backend after each epoch during .fit() & .evaluate() '''\n",
        "\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.model is None:\n",
        "            return\n",
        "\n",
        "        if self.n_classes is None:\n",
        "            self.n_classes = self.model.layers[-1].output_shape[1]\n",
        "\n",
        "        threshold = self.get_threshold(epoch + 1)\n",
        "\n",
        "        if self.monitor in logs:\n",
        "            val_acc = logs[self.monitor]\n",
        "            if val_acc < threshold:\n",
        "                # threshold not met, terminate\n",
        "                print(f'\\nEpoch {(epoch + 1)}: Accuracy ({val_acc}) has not reached the baseline threshold {threshold}, terminating training... \\n')\n",
        "                self.model.stop_training = True\n",
        "\n",
        "                "
      ],
      "metadata": {
        "id": "ziIY3qNmfs-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' TESTING THRESHOLD EVAL '''\n",
        "threshold_multiplier = 0.25\n",
        "diminishing_factor = 0.25\n",
        "n_classes = 10\n",
        "n_epochs = 50\n",
        "\n",
        "thresholds = []\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "\n",
        "    baseline = 1.0 / n_classes     # baseline (random) val_acc\n",
        "    complement_baseline = 1 - baseline\n",
        "    delta_threshold = complement_baseline * threshold_multiplier\n",
        "    base_threshold = baseline + delta_threshold\n",
        "\n",
        "    # epoch-based decaying increase in val_acc threshold\n",
        "    complement_threshold = 1 - base_threshold    # the increase factor's upper limit\n",
        "    growth_denom = (1.0 / complement_threshold) + diminishing_factor * (epoch - 1)\n",
        "    growth_factor = complement_threshold - 1.0 / growth_denom\n",
        "\n",
        "    calculated_threshold = base_threshold + growth_factor\n",
        "\n",
        "    thresholds.append(calculated_threshold)\n",
        "\n",
        "plt.plot(thresholds, '.')\n",
        "plt.show()\n",
        "\n",
        "# print first 5 thresholds\n",
        "print('\\n\\n')\n",
        "[print(f'Epoch {t+1} threshold: {thresholds[t]}') for t in range(0, 5)];\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "4MabBEyDNwZB",
        "outputId": "a3b97a22-f840-44f3-f596-075e96bf9102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAASuklEQVR4nO3de4xc51nH8e/PdqyiUiB1FoR8N7iCiEtLVumi9o9QCHJLlQBFyGlArUSwkGIoUC4pQi0EIS7iKmGQrBC1oICJAhQDkdIKjAqoLt6lLWCH0GWpic0lrutw+YM6Wz/8MeMwbPcyu3tmZ+bM9yNZnnPO653nyOOfXz3nPWdSVUiSxt+2YRcgSWqGgS5JLWGgS1JLGOiS1BIGuiS1xI5hvfFtt91WBw4cGNbbS9JYmpub+0RVTS13bGiBfuDAAWZnZ4f19pI0lpJcXOmYLRdJagkDXZJawkCXpJYw0CWpJQx0SWoJA12SWsJAl6QtNHfxGifOzDN38VrjP3to69AladzNXbzG2YWrzBzaxR37b+1r//2PnOX64g127tjGYw/M/L/jm2WgS1LXegJ6pXBeLbTPLlzl+uINbhS8sHiDswtXDXRJWstK4bzSsfUG9ErhvFpozxzaxc4d23hh8Qa37NjGzKFdjZ6zgS5pLDQxe17t2HoDeqVwXi2079h/K489MLPifzSbZaBLGimDnD3Dym2P9Qb0SuG8Vmjfsf/WxoP8JgNd0sA01fZoavYMzQb0SuE8yNBejYEuadMG3fZoavbcz7FRCuj16ivQkxwBfhnYDjxSVT+95Ph+4FFgCvgk8G1VdanhWiVtkSb61U22PZqcPa91bJytGehJtgMngLuBS8C5JKer6kLPsJ8DfqOq3pPkdcBPAd8+iIIlNWO9a6WbmlXDxmfW4zx73gr9zNDvBOaragEgySngXqA30G8Hvr/7+gzw3iaLlLRx622HNNWvbrrtobX1E+i7gWd7ti8Br14y5qPAN9Npy3wT8LIku6rqau+gJMeAYwD79u3baM2SltFUO6TpfvWktT2GqamLoj8A/EqStwIfAC4Dn146qKpOAicBpqenq6H3liZKE8G9kbXSzqpHXz+BfhnY27O9p7vvRVX1L3Rm6CT5bOBNVfV8U0VKk2iQwb3RtdIG92jrJ9DPAYeTHKQT5EeBN/cOSHIb8MmqugG8g86KF0lrWO+Fyab72IZzu6wZ6FW1mOQ48BSdZYuPVtX5JA8Ds1V1GrgL+KkkRafl8uAAa5bGTlMXJg1uraavHnpVPQk8uWTfO3tePwE80Wxp0vgZ9IVJg1ur8U5RaQOGdWHy5jGDW8sx0KV1GvaFSWklBrq0iuVm4va3NaoMdE289a40Mbg1qgx0TbSNrDQxuDWqDHRNjPW0T2Dtb54xuDVqDHS1znpWoAzz68KkphnoapX1rkBxpYnaxEDX2GpiBQoY2moPA11jqckVKFJbGOgaeeuZibsCRZPMQNdI2+jFTINbk8hA10hY6eaejV7MlCaRga6hW+3mHmfiUv8MdG2p9d7c40xc6p+Bri2zkX44OBOX+mWga8vYD5cGy0DXQCzXWrEfLg2Wga7GrdRacSYuDZaBrk3ZyEVOg1waDANdG7bRi5ySBsNA14Z5kVMaLQa61rTSXZxe5JRGi4GuVa12F6czcWm0bOtnUJIjSZ5JMp/koWWO70tyJsmHk/xNkjc0X6qGYbm2Sq879t/Kg1/zxYa5NALWDPQk24ETwOuB24H7kty+ZNiPAo9X1auAo8CvNl2oBm/u4jVOnJln7uK1F/fdbKtsD17glEZcPy2XO4H5qloASHIKuBe40DOmgM/pvv5c4F+aLFKD59pxafz1E+i7gWd7ti8Br14y5seA9yX5buClwNct94OSHAOOAezbt2+9tWqAXDsujb++euh9uA94d1XtAd4A/GaSz/jZVXWyqqaranpqaqqht1YTbK1I46+fGfplYG/P9p7uvl7fARwBqKoPJnkJcBvwXBNFqlnLLUO0tSKNv34C/RxwOMlBOkF+FHjzkjH/DHwt8O4kXwq8BLjSZKFqxlrLEA1yaXyt2XKpqkXgOPAU8DSd1Sznkzyc5J7usLcD35nko8BvA2+tqhpU0dq4tZYhShpffd1YVFVPAk8u2ffOntcXgNc0W5oGweesSO3lnaItZq9cmiwGekvZK5cmT1PLFjVi7JVLk8dAbynXlUuTx5ZLC9grlwQG+tizVy7pJlsuY85euaSbDPQxZ69c0k22XMacvXJJNxnoY2Kl7/UEe+WSOgz0MbDahU9Juske+hjwwqekfhjoY8ALn5L6YctlDHjhU1I/DPQRs9LFTy98SlqLgT5CvPgpaTPsoY8QL35K2gwDfYR48VPSZthyGSFe/JS0GQb6iPHip6SNsuUiSS1hoA/J3MVrnDgzz9zFa8MuRVJL2HIZApcnShoEZ+hD4PJESYPQV6AnOZLkmSTzSR5a5vgvJvlI99c/JHm++VLbw+WJkgZhzZZLku3ACeBu4BJwLsnpqrpwc0xVfV/P+O8GXjWAWlvD5YmSBqGfHvqdwHxVLQAkOQXcC1xYYfx9wLuaKa+9XJ4oqWn9tFx2A8/2bF/q7vsMSfYDB4E/XeH4sSSzSWavXLmy3lolSato+qLoUeCJqvr0cger6mRVTVfV9NTUVMNvPZpcnihpq/TTcrkM7O3Z3tPdt5yjwIObLaotXJ4oaSv1M0M/BxxOcjDJTjqhfXrpoCRfAtwKfLDZEseXyxMlbaU1A72qFoHjwFPA08DjVXU+ycNJ7ukZehQ4VVU1mFLHj8sTJW2lDCt/p6ena3Z2dijvvZVW+gYiSdqIJHNVNb3cMW/9HzCXJ0raKt76L0ktYaBLUksY6A1xvbmkYbOH3gDXm0saBc7QG+B6c0mjwEBvgOvNJY0CWy4N8HG4kkaBgd4Q15tLGjZbLpLUEga6JLWEgS5JLWGgS1JLGOiS1BIG+jp4e7+kUeayxT55e7+kUecMvU/e3i9p1BnoffL2fkmjzpZLn7y9X9KoM9DXwdv7JY0yWy6S1BIGuiS1hIEuSS3RV6AnOZLkmSTzSR5aYcy3JrmQ5HyS32q2TEnSWta8KJpkO3ACuBu4BJxLcrqqLvSMOQy8A3hNVV1L8vmDKliStLx+Zuh3AvNVtVBV14FTwL1LxnwncKKqrgFU1XPNlilJWks/gb4beLZn+1J3X69XAK9I8pdJziY50lSBw+AzWySNo6bWoe8ADgN3AXuADyT58qp6vndQkmPAMYB9+/Y19NbN8pktksZVPzP0y8Denu093X29LgGnq+qFqvon4B/oBPz/U1Unq2q6qqanpqY2WvNA+cwWSeOqn0A/BxxOcjDJTuAocHrJmPfSmZ2T5DY6LZiFBuvcMj6zRdK4WrPlUlWLSY4DTwHbgUer6nySh4HZqjrdPfb1SS4AnwZ+sKrGcmrrM1skjatU1VDeeHp6umZnZ4fy3pI0rpLMVdX0cse8U1SSWsJAl6SWMNAlqSUMdElqCQNdklrCQJekljDQJaklDHRJaomJDnSfqiipTZp62uLY8amKktpmYmfoPlVRUttMbKD7VEVJbTOxLRefqiipbSY20KET6ga5pLaY2JaLJLWNgS5JLWGgS1JLGOiS1BIGuiS1hIEuSS1hoEtSSxjoktQSBroktYSBLkkt0VegJzmS5Jkk80keWub4W5NcSfKR7q8Hmi9VkrSaNZ/lkmQ7cAK4G7gEnEtyuqouLBn6O1V1fAA1SpL60M8M/U5gvqoWquo6cAq4d7BlSZLWq59A3w0827N9qbtvqTcl+ZskTyTZu9wPSnIsyWyS2StXrmyg3I3xq+YkTYKmLor+IXCgqr4CeD/wnuUGVdXJqpququmpqamG3np1N79q7uff9wz3P3LWUJfUWv0E+mWgd8a9p7vvRVV1tao+1d18BLijmfI2z6+akzQp+gn0c8DhJAeT7ASOAqd7ByT5wp7Ne4Cnmytxc/yqOUmTYs1VLlW1mOQ48BSwHXi0qs4neRiYrarTwPckuQdYBD4JvHWANa+LXzUnaVKkqobyxtPT0zU7OzuU95akcZVkrqqmlzvmnaKS1BIGuiS1hIEuSS1hoEtSSxjoktQSBroktYSBLkktYaBLUksY6JLUEga6JLWEgS5JLWGgS1JLGOiS1BIGuiS1hIEuSS1hoEtSSxjoktQSBroktUSrAn3u4jVOnJln7uK1YZciSVtuzS+JHhdzF69x/yNnub54g507tvHYAzN+IbSkidKaGfrZhatcX7zBjYIXFm9wduHqsEuSpC3VmkCfObSLnTu2sT1wy45tzBzaNeySJGlLtablcsf+W3nsgRnOLlxl5tAu2y2SJk5rAh06oW6QS5pUfbVckhxJ8kyS+SQPrTLuTUkqyXRzJUqS+rFmoCfZDpwAXg/cDtyX5PZlxr0MeBvwoaaLlCStrZ8Z+p3AfFUtVNV14BRw7zLjfgL4GeB/GqxPktSnfgJ9N/Bsz/al7r4XJfkqYG9V/fFqPyjJsSSzSWavXLmy7mIlSSvb9LLFJNuAXwDevtbYqjpZVdNVNT01NbXZt5Yk9egn0C8De3u293T33fQy4MuAP0vycWAGOO2FUUnaWv0E+jngcJKDSXYCR4HTNw9W1X9U1W1VdaCqDgBngXuqanYgFUuSlrVmoFfVInAceAp4Gni8qs4neTjJPYMuUJLUn75uLKqqJ4Enl+x75wpj79p8WZKk9WrNs1wkadIZ6JLUEga6JLWEgS5JLWGgS1JLGOiS1BIGuiS1hIEuSS1hoEtSSxjoktQSBroktYSBLkktYaBLUksY6JLUEga6JLWEgS5JLWGgS1JLjF2gz128xokz88xdvDbsUiRppPT1FXSjYu7iNe5/5CzXF2+wc8c2Hntghjv23zrssiRpJIzVDP3swlWuL97gRsELizc4u3B12CVJ0sgYq0CfObSLnTu2sT1wy45tzBzaNeySJGlkjFXL5Y79t/LYAzOcXbjKzKFdtlskqcdYBTp0Qt0gl6TPNFYtF0nSyvoK9CRHkjyTZD7JQ8sc/64kf5vkI0n+IsntzZcqSVrNmoGeZDtwAng9cDtw3zKB/VtV9eVV9UrgZ4FfaLxSSdKq+pmh3wnMV9VCVV0HTgH39g6oqv/s2XwpUM2VKEnqRz8XRXcDz/ZsXwJevXRQkgeB7wd2Aq9b7gclOQYcA9i3b996a5UkraKxi6JVdaKqvgj4YeBHVxhzsqqmq2p6amqqqbeWJNHfDP0ysLdne09330pOAb+21g+dm5v7RJKLfbz/cm4DPrHBPzvOJvW8YXLP3fOeLP2c9/6VDvQT6OeAw0kO0gnyo8CbewckOVxVH+tufgPwMdZQVRueoieZrarpjf75cTWp5w2Te+6e92TZ7HmvGehVtZjkOPAUsB14tKrOJ3kYmK2q08DxJF8HvABcA96y0YIkSRvT152iVfUk8OSSfe/sef22huuSJK3TuN4penLYBQzJpJ43TO65e96TZVPnnSqXjEtSG4zrDF2StISBLkktMXaBvtaDwtoiyaNJnkvydz37Xp7k/Uk+1v29dc8RTrI3yZkkF5KcT/K27v5Wn3uSlyT5qyQf7Z73j3f3H0zyoe7n/XeS7Bx2rYOQZHuSDyf5o+526887ycd7Hmo42923qc/5WAV6nw8Ka4t3A0eW7HsI+JOqOgz8SXe7bRaBt1fV7cAM8GD377jt5/4p4HVV9ZXAK4EjSWaAnwF+saq+mM6S4O8YYo2D9Dbg6Z7tSTnvr6mqV/asPd/U53ysAp0+HhTWFlX1AeCTS3bfC7yn+/o9wDduaVFboKr+tar+uvv6v+j8I99Ny8+9Ov67u3lL91fReS7SE939rTtvgCR76NyQ+Eh3O0zAea9gU5/zcQv05R4UtntItQzDF1TVv3Zf/xvwBcMsZtCSHABeBXyICTj3btvhI8BzwPuBfwSer6rF7pC2ft5/Cfgh4EZ3exeTcd4FvC/JXPfBhbDJz/nYfQWdOqqqkrR2zWmSzwZ+F/jeqvrPzqSto63nXlWfBl6Z5POA3we+ZMglDVySNwLPVdVckruGXc8We21VXU7y+cD7k/x978GNfM7HbYa+3geFtc2/J/lCgO7vzw25noFIcgudMH+sqn6vu3sizh2gqp4HzgBfDXxekpsTrzZ+3l8D3JPk43RaqK8Dfpn2nzdVdbn7+3N0/gO/k01+zsct0F98UFj3qvdR4PSQa9pKp/m/5+S8BfiDIdYyEN3+6a8DT1dV7zdftfrck0x1Z+Yk+SzgbjrXD84A39Id1rrzrqp3VNWeqjpA59/zn1bV/bT8vJO8NMnLbr4Gvh74Ozb5OR+7O0WTvIFOz+3mg8J+csglDUSS3wbuovM4zX8H3gW8F3gc2AdcBL61qpZeOB1rSV4L/Dnwt/xfT/VH6PTRW3vuSb6CzkWw7XQmWo9X1cNJDtGZub4c+DDwbVX1qeFVOjjdlssPVNUb237e3fP7/e7mDjpf4/mTSXaxic/52AW6JGl549ZykSStwECXpJYw0CWpJQx0SWoJA12SWsJAl6SWMNAlqSX+F34iXi2kUGTrAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "Epoch 1 threshold: 0.325\n",
            "Epoch 2 threshold: 0.4224598930481284\n",
            "Epoch 3 threshold: 0.4953271028037383\n",
            "Epoch 4 threshold: 0.5518672199170125\n",
            "Epoch 5 threshold: 0.5970149253731344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Momentum Augmentation"
      ],
      "metadata": {
        "id": "cj1xmJPigB_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MomentumAugmentation(Callback):\n",
        "    ''' Calculates the momentum's moving average of the parent model '''\n",
        "\n",
        "    def __init__(self, monitor='val_sparse_categorical_accuracy'):\n",
        "        ''' Initialize MA '''\n",
        "\n",
        "        super(MomentumAugmentation, self).__init__()\n",
        "        self.monitor = monitor\n",
        "\n",
        "    \n",
        "    def get_momentum(self, epoch, acc):\n",
        "        ''' Calculates the momentums based on the given accuracies and epochs '''\n",
        "\n",
        "        if epoch < 2:\n",
        "            # momentum = acc at Îµ < 3\n",
        "            return acc\n",
        "\n",
        "        delta_1 = acc - self.model.momentum[epoch - 1][0]\n",
        "        delta_2 = self.model.momentum[epoch - 1][0] - self.model.momentum[epoch - 2][0]\n",
        "\n",
        "        if delta_2 == 0.0:\n",
        "            # avoid division by 0\n",
        "            # if previous 2 accuracies are somehow exactly the same (very unlikely) => 0 momentum\n",
        "            return 0.0\n",
        "\n",
        "        current_momentum = delta_1 / delta_2\n",
        "\n",
        "        return (acc, current_momentum)\n",
        "\n",
        "    \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        ''' Called by Keras backend after each epoch during .fit() & .evaluate() '''\n",
        "\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.model is None:\n",
        "            return\n",
        "\n",
        "        if not hasattr(self.model, 'momentum'):\n",
        "            self.model.momentum = {}\n",
        "\n",
        "        if self.monitor in logs:\n",
        "            val_acc = logs[self.monitor]\n",
        "            \n",
        "            self.model.momentum[epoch] = self.get_momentum(epoch, val_acc)\n",
        "            \n",
        "    "
      ],
      "metadata": {
        "id": "GSHOs7sHgHTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQl0Y7o5iBM8"
      },
      "source": [
        "#### NAS Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkz4zLd92Ume"
      },
      "outputs": [],
      "source": [
        "class HiveNAS(object):\n",
        "    ''' \n",
        "        An interface that combines the Search Space + Evaluation Strategy \n",
        "        for the Artificial Bee Colony algorithm\n",
        "    '''\n",
        "\n",
        "    search_space = None\n",
        "    eval_strategy = None\n",
        "\n",
        "    def __init__(self, \n",
        "                 space_config=None,\n",
        "                 eval_config=None):\n",
        "        ''' Initializes the search space and evaluator '''\n",
        "\n",
        "        space_config = space_config or Params.search_space_config()\n",
        "        eval_config = eval_config or Params.evaluation_strategy_config()\n",
        "\n",
        "        HiveNAS.search_space = NASSearchSpace(space_config)\n",
        "        HiveNAS.eval_strategy = NASEval(eval_config)\n",
        "\n",
        "\n",
        "    def sample(self):\n",
        "        ''' Samples new random candidate architecture from the search space '''\n",
        "\n",
        "        return HiveNAS.search_space.sample()\n",
        "\n",
        "\n",
        "    def evaluate(self, candidate):\n",
        "        ''' Evaluates a given candidate architecture; returns loss value '''\n",
        "\n",
        "        formatted = HiveNAS.search_space.eval_format(candidate)\n",
        "        res = HiveNAS.eval_strategy.evaluate(formatted)\n",
        "\n",
        "        # housekeeping\n",
        "        K.clear_session()\n",
        "        gc.collect()\n",
        "        \n",
        "        return res\n",
        "\n",
        "\n",
        "    def get_neighbor(self, orig_arch):\n",
        "        ''' Returns a random architecture with 1 op diff to the given candidate '''\n",
        "\n",
        "        return HiveNAS.search_space.get_neighbor(orig_arch)\n",
        "\n",
        "\n",
        "    def fully_train_best_model(self, from_arch=True):\n",
        "        '''\n",
        "            Fully-train best-performing model\n",
        "            (relies on paths set in Params)\n",
        "        '''\n",
        "\n",
        "        # check existence of results file\n",
        "        filename = f'{Params[\"CONFIG_VERSION\"]}.csv'\n",
        "        results_file = os.path.join(Params.get_results_path(), filename)\n",
        "        FileHandler.path_must_exist(results_file)    # breaks if file does not exist\n",
        "\n",
        "        # extract best fitness weight file\n",
        "        results_df = pd.read_csv(results_file, header=0, index_col=0)\n",
        "        weight_file = results_df.loc[results_df['fitness'] == results_df['fitness'].max(), 'weights_filename'].values[0]\n",
        "        arch = results_df.loc[results_df['fitness'] == results_df['fitness'].max(), 'candidate'].values[0]\n",
        "\n",
        "        print(f'\\nFound best-performing model {{{arch}}} with a fitness score of {results_df[\"fitness\"].max()}\\n')\n",
        "        \n",
        "        # housekeeping -> results_df no longer needed and is potentially large\n",
        "        del results_df\n",
        "        gc.collect()\n",
        "\n",
        "        if from_arch:\n",
        "            # Retrains from scratch given the network arch\n",
        "            arch = formatted = HiveNAS.search_space.eval_format(arch)\n",
        "            return HiveNAS.eval_strategy.fully_train(arch=arch)\n",
        "\n",
        "        # check existence of weight file\n",
        "        weight_file = os.path.join(Params.get_results_path(), Params['WEIGHT_FILES_SUBPATH'], weight_file)\n",
        "        FileHandler.path_must_exist(weight_file)    # breaks if file does not exist\n",
        "        \n",
        "\n",
        "        # Continues training from saved h5 model (often results in lower fitness)\n",
        "        return HiveNAS.eval_strategy.fully_train(model_file=weight_file)\n",
        "\n",
        "\n",
        "    @property \n",
        "    def is_minimize(self):\n",
        "        ''' \n",
        "            Used by the ABC algorithm to determine whether this is \n",
        "            a minimization or maximization problem (we're maximizing accuracy)\n",
        "        '''\n",
        "\n",
        "        return False\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "interface = HiveNAS()\n"
      ],
      "metadata": {
        "id": "BekYnhYSdAYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ss = interface.sample()\n",
        "ss"
      ],
      "metadata": {
        "id": "HZwvpG6EdJmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interface.evaluate(ss)"
      ],
      "metadata": {
        "id": "GIiQYic2dqIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "momentum = {0: (0.33500000834465027, 1.0), 1: (0.4878999888896942, 1.0), 2: (0.5139999985694885, 0.1706998888211455), 3: (0.5547999739646912, 1.5632168683365857), 4: (0.5895000100135803, 0.8504915925260406)}\n",
        "sum([x[1] for k,x in momentum.items()]) / len(momentum)"
      ],
      "metadata": {
        "id": "YKHIZ_vuL9Ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntNIB5vO8T5a"
      },
      "source": [
        "### Numerical Optimization Benchmarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5Hg6jF_g_d9"
      },
      "source": [
        "#### NumericalBenchmark Base Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KccufdV08X3b"
      },
      "outputs": [],
      "source": [
        "class NumericalBenchmark(ABC):\n",
        "    ''' Abstract class for Numerical Optimization Benchmarks '''\n",
        "\n",
        "    def __init__(self, dim, minv, maxv, minimization):\n",
        "        self.dim = dim\n",
        "        self.minv = minv\n",
        "        self.maxv = maxv\n",
        "        self.minimization = minimization\n",
        "\n",
        "\n",
        "    def sample(self):\n",
        "        ''' Samples a random point from the objective function '''\n",
        "\n",
        "        return np.random.uniform(low=self.minv, high=self.maxv, \\\n",
        "                                 size=self.dim)\n",
        "        \n",
        "\n",
        "    def get_neighbor(self, pos):\n",
        "        ''' Finds a random neighbor by nudging 1 component of the pos by phi '''\n",
        "\n",
        "        op = np.random.choice(pos)\n",
        "        phi = np.random.uniform(low=-1, high=1, size=len(pos))\n",
        "        neighbor_pos = pos + (pos - op) * phi\n",
        "\n",
        "        return self.__eval_boundary(neighbor_pos)\n",
        "    \n",
        "\n",
        "    def __eval_boundary(self, n_pos):\n",
        "        '''\n",
        "            Ensures the newly sampled position (neighbor) lies within the\n",
        "            Search Space boundaries\n",
        "        '''\n",
        "\n",
        "        if (n_pos < self.minv).any() or \\\n",
        "        (n_pos > self.maxv).any():\n",
        "            n_pos[n_pos < self.minv] = self.minv\n",
        "            n_pos[n_pos > self.maxv] = self.maxv\n",
        "        return n_pos\n",
        "\n",
        "\n",
        "    @property\n",
        "    def minimum(self):\n",
        "        return self.minv\n",
        "\n",
        "\n",
        "    @property\n",
        "    def maximum(self):\n",
        "        return self.maxv\n",
        "\n",
        "\n",
        "    @property \n",
        "    def is_minimize(self):\n",
        "        return self.minimization\n",
        "\n",
        "\n",
        "    @abstractmethod\n",
        "    def evaluate(self, pos):\n",
        "        raise NotImplementedError()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGYJZBG-gH6s"
      },
      "source": [
        "#### Sphere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70x3Oco4-csV"
      },
      "outputs": [],
      "source": [
        "class Sphere(NumericalBenchmark):\n",
        "    ''' Sphere optimization benchmark -- [ Î£ Xi^2 , for i=1 in dim ] '''\n",
        "    \n",
        "    def __init__(self, dim, is_minimization=True):\n",
        "        super(Sphere, self).__init__(dim, -100.0, 100.0, is_minimization)\n",
        "        \n",
        "    \n",
        "    def evaluate(self, pos):\n",
        "        return {\n",
        "            'fitness': sum(np.power(pos, 2)),\n",
        "            'epochs': 1,\n",
        "            'filename': '',\n",
        "            'params': self.dim\n",
        "        }\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcAjG8NJgKry"
      },
      "source": [
        "#### Rosenbrock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dm2x7F2V_VsV"
      },
      "outputs": [],
      "source": [
        "class Rosenbrock(NumericalBenchmark):\n",
        "    ''' \n",
        "        -- Rosenbrock optimization benchmark -- \n",
        "        [ Î£ { 100(Xi+1 - Xi)^2 + (Xi - 1)^2 } , for i=1 in dim - 1 ] \n",
        "    '''\n",
        "\n",
        "    def __init__(self, dim):\n",
        "        super(Rosenbrock, self).__init__(dim, -30.0, 30.0, True)\n",
        "\n",
        "\n",
        "    def evaluate(self, pos):\n",
        "        ''' Scipy implementation '''\n",
        "\n",
        "        return {\n",
        "            'fitness': optimize.rosen(pos),\n",
        "            'epochs': 1,\n",
        "            'filename': '',\n",
        "            'params': self.dim\n",
        "        }\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDcYRCilw2bZ"
      },
      "source": [
        "### Artificial Bee Colony Components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYG3yKSEgOC8"
      },
      "source": [
        "#### FoodSource"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsJNUMOiVRwx"
      },
      "outputs": [],
      "source": [
        "class FoodSource:\n",
        "    \n",
        "    def __init__(self, position=None, fitness=None):\n",
        "        ''' \n",
        "            Data structure containing a FoodSource (position on the \n",
        "            optimization surface) and its fitness value\n",
        "        '''\n",
        "\n",
        "        self.pos = position\n",
        "        self.fit = fitness\n",
        "        self.eval_time = 0.0\n",
        "\n",
        "\n",
        "    def encode_position(self):\n",
        "        ''' Returns a stripped string-encoded position hash for dicts '''\n",
        "\n",
        "        return str(self.pos).replace(' ', '')\n",
        "\n",
        "\n",
        "    # --- Setters & Getters --- #\n",
        "\n",
        "    @property\n",
        "    def position(self):\n",
        "        return self.pos\n",
        "    \n",
        "\n",
        "    @property\n",
        "    def fitness(self):\n",
        "        return self.fit\n",
        "\n",
        "\n",
        "    @property\n",
        "    def time(self):\n",
        "        return self.eval_time\n",
        "\n",
        "\n",
        "    @position.setter\n",
        "    def position(self, value):\n",
        "        self.pos = value\n",
        "\n",
        "\n",
        "    @fitness.setter\n",
        "    def fitness(self, value):\n",
        "        self.fit = value\n",
        "\n",
        "\n",
        "    @time.setter\n",
        "    def time(self, value):\n",
        "        self.eval_time = value\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "        ''' For logging/debugging purposes '''\n",
        "\n",
        "        return 'position: {}, fitness: {}, evaluation time: {}'.format(self.pos, \n",
        "                                                                       self.fit,\n",
        "                                                                       self.eval_time)\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        ''' For logging/debugging purposes '''\n",
        "\n",
        "        return str(self)\n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uqPXJlagRWu"
      },
      "source": [
        "#### ArtificialBee Base Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPkKWP7ow5Gq"
      },
      "outputs": [],
      "source": [
        "class ArtificialBee(ABC):\n",
        "    ''' \n",
        "        Abstract class for Employee & Onlooker Bees\n",
        "    '''\n",
        "\n",
        "    def __init__(self, food_source, id):\n",
        "        self.food_source = food_source\n",
        "        self.id = id if id is not None else -1\n",
        "    \n",
        "\n",
        "    def get_random_neighbor(self, obj_interface):\n",
        "        '''\n",
        "            Finds a random neighbor in the vicinity of the parent \n",
        "            Parent(Onlooker) = Employee,\n",
        "            Parent(Employee) = Scout\n",
        "            Given by [1]:\n",
        "\n",
        "                Xmi = Li + rand(0, 1) âˆ— (Ui âˆ’ Li)   => Initial FoodSource\n",
        "                                                       (Scout)\n",
        "                Ï…mi = Xmi + Ï•mi(Xmi âˆ’ Xki)          => Neighboring FoodSource \n",
        "                                                       (Employee/Onlooker)\n",
        "\n",
        "                Where Ï…mi is a neighboring FoodSource\n",
        "                (definition of \"neighboring\" given in [2]; \n",
        "                TLDR - in numerical and continuous optimization problems, \n",
        "                a dimensional component is incremented/decremented. \n",
        "                In NAS context, it is a 1-operation difference per network)\n",
        "\n",
        "\n",
        "            [1] Karaboga, D., & Basturk, B. (2007). A powerful and efficient \n",
        "            algorithm for numerical function optimization: artificial bee \n",
        "            colony (ABC) algorithm. Journal of global optimization, 39(3), \n",
        "            459-471.\n",
        "\n",
        "            [2] White, C., Nolen, S., & Savani, Y. (2021, December). \n",
        "            Exploring the loss landscape in neural architecture search. \n",
        "            In Uncertainty in Artificial Intelligence (pp. 654-664). PMLR.\n",
        "        '''\n",
        "        \n",
        "        pos = self.get_center_fs().position\n",
        "        neighbor_pos = obj_interface.get_neighbor(pos)\n",
        "\n",
        "        return FoodSource(neighbor_pos)\n",
        "\n",
        "\n",
        "    def is_evaluated(self):\n",
        "        ''' \n",
        "            Checks if food source is evaluated for\n",
        "            solution tracking purposes\n",
        "        '''\n",
        "\n",
        "        return self.food_source is not None and \\\n",
        "        self.food_source.fitness is not None\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_center_fs(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "    @abstractmethod\n",
        "    def evaluate(self, obj_interface):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "    @abstractmethod\n",
        "    def search(self, obj_interface):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        ''' For logging/debugging purposes '''\n",
        "        return str(self)\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1BmrcYbgV7w"
      },
      "source": [
        "#### EmployeeBee"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjZG9UsKxGjD"
      },
      "outputs": [],
      "source": [
        "class EmployeeBee(ArtificialBee):\n",
        "\n",
        "    id_tracker = 0\n",
        "\n",
        "    def __init__(self, food_source):\n",
        "        super(EmployeeBee, self).__init__(food_source, EmployeeBee.id_tracker)\n",
        "        EmployeeBee.id_tracker += 1\n",
        "        self.trials = 0\n",
        "        self.center_fs = food_source   # center_fs can be greedy-selected by\n",
        "                                       # child onlookers (i.e the new\n",
        "                                       # optimization center)\n",
        "\n",
        "\n",
        "    def search(self, obj_interface):\n",
        "        ''' \n",
        "            Explore new random position (near previously-sampled position)\n",
        "        '''\n",
        "        \n",
        "        if self.food_source.fitness is None:\n",
        "            # unevaluated (first iteration after abandonment reset)\n",
        "            return\n",
        "\n",
        "        # find neighbor near food_source in the bee's memory\n",
        "        self.food_source = self.get_random_neighbor(obj_interface)\n",
        "\n",
        "\n",
        "    def reset(self, new_fs):\n",
        "        ''' Resets EmployeeBee once abandonment limit is reached '''\n",
        "\n",
        "        self.trials = 0\n",
        "        self.food_source = new_fs\n",
        "        self.center_fs = new_fs\n",
        "\n",
        "\n",
        "    def calculate_fitness(self):\n",
        "        '''\n",
        "            Calculate fitness of an EmployeeBee; given by:\n",
        "\n",
        "                         âŽ§ 1 / (1 + Fm(Xmâ†’))       if  Fm(Xmâ†’)â‰¥0\n",
        "            Fit_m(Xmâ†’)=  âŽ¨\n",
        "                         âŽ© 1 + abs(Fm(Xmâ†’))        if  Fm(Xmâ†’)<0\n",
        "        '''\n",
        "\n",
        "        fitm = 0\n",
        "        if self.center_fs.fitness >= 0:\n",
        "            fitm = 1 / (1 + self.center_fs.fitness)\n",
        "        else:\n",
        "            fitm = 1 + np.abs(self.center_fs.fitness)\n",
        "        \n",
        "        return fitm\n",
        "    \n",
        "\n",
        "    def compute_probability(self, sum_fitness):\n",
        "        '''\n",
        "            Calculate probability of an EmployeeBee being chosen by\n",
        "            an OnlookerBee based on Fitess values; given by:\n",
        "\n",
        "                Pn = Fit_n(Xnâ†’) / [âˆ‘ (Fit_m(Xmâ†’)) for all m]\n",
        "        '''\n",
        "\n",
        "        return self.calculate_fitness() / sum_fitness\n",
        "\n",
        "    \n",
        "    def evaluate(self, obj_interface, itr):\n",
        "        ''' Evaluates sampled position and increments trial counter '''\n",
        "\n",
        "        Logger.evaluation_log('EmployeeBee', self.id, self.food_source.position)\n",
        "        t = time.time()\n",
        "\n",
        "        res = obj_interface.evaluate(self.food_source.position)\n",
        "        # unpack\n",
        "        self.food_source.fitness = res['fitness']\n",
        "        epochs = res['epochs']\n",
        "        weights_filename = res['filename']\n",
        "        params = res['params']\n",
        "        momentum = res['momentum']\n",
        "\n",
        "        self.food_source.time = time.time() - t\n",
        "        # ACT early bandonment (ACT enabled and network could not pass epoch 1)\n",
        "        abandon_early = Params['TERMINATION_THRESHOLD_FACTOR'] > 0.0 and epochs <= 1\n",
        "        self.trials += 1 if not abandon_early else Params['ABANDONMENT_LIMIT']\n",
        "\n",
        "        if self.center_fs.fitness is None:\n",
        "            # Check if this evaluation is the first in its area \n",
        "            # (Iteration 1 after reset; i.e no need to greedy-select)\n",
        "            self.center_fs = deepcopy(self.food_source)\n",
        "        else:\n",
        "            # Greedy select for iterations 2 ... Abandonment Limit\n",
        "            self.greedy_select(self.food_source, obj_interface.is_minimize)\n",
        "\n",
        "        # save data\n",
        "        series = pd.Series({\n",
        "            'bee_type': type(self).__name__,\n",
        "            'bee_id': self.id,\n",
        "            'bee_parent': '-',\n",
        "            'itr': itr,\n",
        "            'candidate': self.food_source.position,\n",
        "            'fitness': self.food_source.fitness,\n",
        "            'center_fitness': self.center_fs.fitness,\n",
        "            'momentum': sum([x[1] for _,x in momentum.items()]),\n",
        "            'epochs': epochs,\n",
        "            'params': params,\n",
        "            'weights_filename': weights_filename,\n",
        "            'time': self.food_source.time\n",
        "        })\n",
        "        return series\n",
        "\n",
        "\n",
        "    def greedy_select(self, n_food_source, is_minimize):\n",
        "        ''' Update best FoodSource to minimize or maximize fitness '''\n",
        "\n",
        "        if ((self.center_fs.fitness < n_food_source.fitness) and not is_minimize) or \\\n",
        "        ((self.center_fs.fitness > n_food_source.fitness) and is_minimize):\n",
        "            self.center_fs.position = n_food_source.position\n",
        "            self.center_fs.fitness = n_food_source.fitness\n",
        "        \n",
        "\n",
        "    def get_center_fs(self):\n",
        "        ''' Returns the center food_source '''\n",
        "\n",
        "        return self.center_fs\n",
        "        \n",
        "\n",
        "    def __str__(self):\n",
        "        ''' For logging/debugging purposes '''\n",
        "\n",
        "        return 'EmployeeBee {} -- FS: {}, trials: {}'.format(self.id, \\\n",
        "                                                             self.food_source, \\\n",
        "                                                             self.trials)\n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adQwk052ga1q"
      },
      "source": [
        "#### OnlookerBee"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIwN0V4PxDQX"
      },
      "outputs": [],
      "source": [
        "class OnlookerBee(ArtificialBee):\n",
        "\n",
        "    id_tracker = 0\n",
        "\n",
        "    def __init__(self):\n",
        "        super(OnlookerBee, self).__init__(None, OnlookerBee.id_tracker)\n",
        "        OnlookerBee.id_tracker += 1\n",
        "        self.employee = None\n",
        "\n",
        "\n",
        "    def search(self, obj_interface):\n",
        "        ''' \n",
        "            Exploit position (near a random EmployeeBee chosen according \n",
        "            to computed probability)\n",
        "        '''\n",
        "\n",
        "        self.food_source = self.get_random_neighbor(obj_interface)\n",
        "\n",
        "\n",
        "    def assign_employee(self, employee):\n",
        "        ''' Assigns an EmployeeBee to the Onlooker for neighbor-search '''\n",
        "\n",
        "        self.employee = employee\n",
        "        self.food_source = FoodSource(self.employee.food_source.position)\n",
        "\n",
        "    \n",
        "    def evaluate(self, obj_interface, itr):\n",
        "        ''' \n",
        "            Evaluates sampled position and increments employee's trial counter\n",
        "        '''\n",
        "\n",
        "        Logger.evaluation_log('OnlookerBee', self.id, self.food_source.position)\n",
        "        t = time.time()\n",
        "        \n",
        "        res = obj_interface.evaluate(self.food_source.position)\n",
        "        # unpack\n",
        "        self.food_source.fitness = res['fitness']\n",
        "        epochs = res['epochs']\n",
        "        weights_filename = res['filename']\n",
        "        params = res['params']\n",
        "        momentum = res['momentum']\n",
        "\n",
        "        self.food_source.time = time.time() - t\n",
        "        self.employee.trials += 1\n",
        "        self.employee.greedy_select(self.food_source, obj_interface.is_minimize)\n",
        "\n",
        "        series = pd.Series({\n",
        "            'bee_type': type(self).__name__,\n",
        "            'bee_id': self.id,\n",
        "            'bee_parent': self.employee.id,\n",
        "            'itr': itr,\n",
        "            'candidate': self.food_source.position,\n",
        "            'fitness': self.food_source.fitness,\n",
        "            'center_fitness': self.get_center_fs().fitness,\n",
        "            'momentum': sum([x[1] for _,x in momentum.items()]) / len(momentum),\n",
        "            'epochs': epochs,\n",
        "            'params': params,\n",
        "            'weights_filename': weights_filename,\n",
        "            'time': self.food_source.time\n",
        "        })\n",
        "        return series\n",
        "        \n",
        "\n",
        "    def get_center_fs(self):\n",
        "        ''' Returns the parent's center food_source '''\n",
        "        \n",
        "        return self.employee.center_fs\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "        ''' For logging/debugging purposes '''\n",
        "\n",
        "        return 'OnlookerBee {} -> Parent Employee ({}) -- FS: {}'\\\n",
        "        .format(self.id, self.employee.id, self.food_source)\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZibiU5kTgd5U"
      },
      "source": [
        "#### ScoutBee"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxC8POuNyH9U"
      },
      "outputs": [],
      "source": [
        "class ScoutBee:\n",
        "    ''' \n",
        "        Scout Bees static class, \n",
        "        responsible for initializing random FoodSources position\n",
        "        and reseting employed bees when abandonment limit is reached\n",
        "    '''\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def sample(obj_interface):\n",
        "        ''' Sample a random point from the objective function '''\n",
        "\n",
        "        return FoodSource(obj_interface.sample())\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def check_employee_trials(employees, obj_interface):\n",
        "        ''' Check if EmployeeBees reached abandonment limit '''\n",
        "\n",
        "        for employee in employees:\n",
        "            if employee.trials >= Params['ABANDONMENT_LIMIT']:\n",
        "                # Reset EmployeeBees to a new ScoutBee position\n",
        "                employee.reset(ScoutBee.sample(obj_interface))\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufPCYNFeggVR"
      },
      "source": [
        "#### ABC Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zWzCTChHSOr"
      },
      "outputs": [],
      "source": [
        "class ArtificialBeeColony:\n",
        "    '''\n",
        "        Main ABC optimizer\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, obj_interface, \\\n",
        "                 colony_size=Params['COLONY_SIZE'], \\\n",
        "                 employee_onlooker_ratio=Params['EMPLOYEE_ONLOOKER_RATIO']):\n",
        "        ''' Initializes the ABC algorithm '''\n",
        "\n",
        "        self.obj_interface = obj_interface    # Encapsulates the Search Space + \n",
        "                                              # Evaluation Strategy\n",
        "        self.colony_size = colony_size\n",
        "        self.eo_colony_ratio = employee_onlooker_ratio\n",
        "        self.scouts_count = int(self.colony_size * self.eo_colony_ratio)\n",
        "\n",
        "\n",
        "    def __init_scouts(self):\n",
        "        ''' \n",
        "            Instantiate Scout bees and sample random positions \n",
        "            (does not evaluate fitness) \n",
        "        '''\n",
        "\n",
        "        for _ in range(self.scouts_count):\n",
        "            self.scouts.append(ScoutBee.sample(self.obj_interface))\n",
        "    \n",
        "\n",
        "    def __init_employees(self):\n",
        "        ''' Instantiate Employee bees and assign a Scout position to each '''\n",
        "\n",
        "        # Floor of (colony_size * ratio)\n",
        "        employee_count = int(self.colony_size * self.eo_colony_ratio)\n",
        "        \n",
        "        for itr in range(employee_count):\n",
        "            # Split scouts evenly among employees\n",
        "            scout = self.scouts[int(itr / (employee_count / \\\n",
        "                                           self.scouts_count))]\n",
        "            self.employees.append(EmployeeBee(scout))\n",
        "\n",
        "\n",
        "    def __init_onlookers(self):\n",
        "        ''' \n",
        "            Instantiate Onlooker bees (assigning Employees occurs after \n",
        "            evaluation and probability calculation) \n",
        "        '''\n",
        "\n",
        "        onlooker_count = self.colony_size - int(self.colony_size * self.eo_colony_ratio)\n",
        "        \n",
        "        for itr in range(onlooker_count):\n",
        "            self.onlookers.append(OnlookerBee())\n",
        "\n",
        "    \n",
        "    def __employee_bee_phase(self, itr):\n",
        "        ''' \n",
        "            Evaluate Scout-initialized position after reset()\n",
        "            or Search + Evaluate neighbor every subsequent iteration\n",
        "            until abandonment limit\n",
        "        '''\n",
        "        \n",
        "        # Search and evaluate new or existing neighbor\n",
        "        for employee in self.employees:\n",
        "            # Ignored if unevaluated position exists (i.e Scout-sampled)\n",
        "            employee.search(self.obj_interface)\n",
        "\n",
        "            fs = employee.food_source\n",
        "            \n",
        "            if fs is None or fs.encode_position() not in self.results_df['candidate']:\n",
        "                # Evaluate employee position\n",
        "                series = employee.evaluate(self.obj_interface, itr)\n",
        "                self.__save_results(series)\n",
        "            else:\n",
        "                # Already evaluated\n",
        "                fs.fitness = self.results_df[self.results_df['candidate'] == fs.encode_position()]['fitness'].values[0]\n",
        "                employee.greedy_select(fs, self.obj_interface.is_minimize)\n",
        "                # resampling the same candidate should count as a trial towards the abandonment limit?\n",
        "                # onlooker.employee.trials += 1     # to avoid being stuck\n",
        "\n",
        "\n",
        "    def __onlooker_bee_phase(self, itr):\n",
        "        ''' \n",
        "            Assign each Onlooker to an Employee, \n",
        "            then Search + Evaluate a random neighbor\n",
        "        '''\n",
        "\n",
        "        # Calculate Employee probability\n",
        "        sum_fitness = sum([employee.calculate_fitness() \\\n",
        "                        for employee in self.employees])\n",
        "        \n",
        "        # Sum(probabilities) â‰ˆ 1.0\n",
        "        probabilities = list(map(lambda employee: \\\n",
        "                                 employee.compute_probability(sum_fitness), \\\n",
        "                                 self.employees))\n",
        "\n",
        "        if not self.obj_interface.is_minimize:\n",
        "            # inverse weights to maximize the objective\n",
        "            # (assign more onlookers to higher fitness scores)\n",
        "\n",
        "            # reciprocals\n",
        "            probabilities = [1.0 / prob for prob in probabilities]\n",
        "\n",
        "            # normalize\n",
        "            sum_probs = sum(probabilities)\n",
        "            probabilities = [prob / sum_probs for prob in probabilities]\n",
        "\n",
        "        # Assign EmployeeBees to OnlookerBees, search, and evaluate neighbor\n",
        "        for onlooker in self.onlookers:\n",
        "            # Assign EmployeeBees to OnlookerBees\n",
        "            emp_idx = np.random.choice(len(self.employees), p=probabilities)\n",
        "\n",
        "            onlooker.assign_employee(self.employees[emp_idx])\n",
        "\n",
        "            # Search for a new random neighbor\n",
        "            onlooker.search(self.obj_interface)\n",
        "\n",
        "            fs = onlooker.food_source\n",
        "            \n",
        "            if fs is None or fs.encode_position() not in self.results_df['candidate']:\n",
        "                # Evaluate employee position\n",
        "                series = onlooker.evaluate(self.obj_interface, itr)\n",
        "                self.__save_results(series)\n",
        "            else:\n",
        "                # Already evaluated\n",
        "                fs.fitness = self.results_df[self.results_df['candidate'] == fs.encode_position()]['fitness'].values[0]\n",
        "                onlooker.employee.greedy_select(fs, self.obj_interface.is_minimize)\n",
        "                # resampling the same candidate should count as a trial towards the abandonment limit?\n",
        "                # onlooker.employee.trials += 1     # to avoid being stuck\n",
        "\n",
        "\n",
        "    def __scout_bee_phase(self):\n",
        "        ''' \n",
        "            Check abandonment limits and rest employees accordingly\n",
        "        '''\n",
        "\n",
        "        ScoutBee.check_employee_trials(self.employees, self.obj_interface)\n",
        "\n",
        "\n",
        "    def __save_results(self, series):\n",
        "        '''\n",
        "            Save results dataframe\n",
        "        '''\n",
        "\n",
        "        if self.total_evals % Params['RESULTS_SAVE_FREQUENCY'] == 0:\n",
        "            self.results_df = self.results_df.append(series, ignore_index=True)\n",
        "            \n",
        "            filename = f'{Params[\"CONFIG_VERSION\"]}.csv'\n",
        "            if FileHandler.save_df(self.results_df, \n",
        "                                   Params.get_results_path(), \n",
        "                                   filename):\n",
        "                Logger.filesave_log(series['candidate'], series['weights_filename'])\n",
        "\n",
        "\n",
        "    def __momentum_phase(self):\n",
        "        ''' \n",
        "            Momentum Augmentation\n",
        "            \n",
        "        '''\n",
        "\n",
        "        pass\n",
        "\n",
        "\n",
        "    def __reset_all(self):\n",
        "        ''' Resets the ABC algorithm '''\n",
        "\n",
        "        self.scouts = []            # List of FoodSources initially sampled\n",
        "        self.employees = []\n",
        "        self.onlookers = []\n",
        "\n",
        "        EmployeeBee.id_tracker = 0\n",
        "        OnlookerBee.id_tracker = 0\n",
        "\n",
        "        # init results dataframe\n",
        "        filename = f'_{Params[\"CONFIG_VERSION\"]}.csv'\n",
        "        results_file = os.path.join(Params.get_results_path(), filename)\n",
        "        \n",
        "        # resume from previously saved file if it exists\n",
        "        if Params['RESUME_FROM_RESULTS_FILE']:\n",
        "            self.results_df = FileHandler.load_df(results_file)     # loads empty df if file not found\n",
        "\n",
        "        else:\n",
        "            cols = ['bee_type'\n",
        "            'bee_id',\n",
        "            'bee_parent',\n",
        "            'itr',\n",
        "            'candidate',\n",
        "            'fitness',\n",
        "            'center_fitness',\n",
        "            'epochs',\n",
        "            'params',\n",
        "            'weights_filename',\n",
        "            'time']\n",
        "            \n",
        "            self.results_df = pd.DataFrame(columns=cols)\n",
        "\n",
        "        self.total_evals = len(self.results_df.index)\n",
        "\n",
        "\n",
        "    def optimize(self):\n",
        "        ''' Initialize ABC algorithm'''\n",
        "\n",
        "        Params.export_yaml(Params.get_results_path(), \n",
        "                           f'{Params[\"CONFIG_VERSION\"]}.yaml')\n",
        "\n",
        "        Logger.start_log()\n",
        "\n",
        "        self.__reset_all()\n",
        "        self.__init_scouts()\n",
        "        self.__init_employees()\n",
        "        self.__init_onlookers()\n",
        "        start_time = time.time()\n",
        "\n",
        "        fitness_selector = max if not self.obj_interface.is_minimize else min\n",
        "\n",
        "        ''' Optimization loop '''\n",
        "        for itr in range(Params['ITERATIONS_COUNT']):\n",
        "            self.__employee_bee_phase(itr)\n",
        "            self.__onlooker_bee_phase(itr)\n",
        "            self.__momentum_phase()\n",
        "            self.__scout_bee_phase()\n",
        "            \n",
        "            best_fitness = fitness_selector(self.results_df['fitness'].tolist())\n",
        "\n",
        "            if itr % 1 == 0:\n",
        "                Logger.status(itr,\n",
        "                          'Best fitness: {}, Total time (s): {}'.format(best_fitness,\n",
        "                                                                        time.time() - start_time))\n",
        "        \n",
        "        Logger.end_log()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBY5aOEZgo0Z"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxYhgNtsJW7i"
      },
      "outputs": [],
      "source": [
        "''' Run HiveNAS '''\n",
        "\n",
        "Logger.EVALUATION_LOGGING = True\n",
        "\n",
        "if Params['OPTIMIZATION_OBJECTIVE'] == 'HiveNAS':\n",
        "    objective_interface = HiveNAS()     \n",
        "elif Params['OPTIMIZATION_OBJECTIVE'] == 'Sphere_min':\n",
        "    objective_interface = Sphere(10)\n",
        "elif Params['OPTIMIZATION_OBJECTIVE'] == 'Sphere_max':\n",
        "    objective_interface = Sphere(10, False)\n",
        "elif Params['OPTIMIZATION_OBJECTIVE'] == 'Rosenbrock':\n",
        "    objective_interface = Rosenbrock(2)\n",
        "\n",
        "abc = ArtificialBeeColony(objective_interface)\n",
        "\n",
        "abc.optimize()\n",
        "\n",
        "# Disconnect runtime / free up GPU instance\n",
        "!kill -9 -1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' Fully-train best-performing model previously found through HiveNAS '''\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "Logger.start_log()\n",
        "\n",
        "res = HiveNAS().fully_train_best_model()\n",
        "\n",
        "Logger.end_log()\n",
        "\n",
        "print(res)\n",
        "\n",
        "# Disconnect runtime / free up GPU instance\n",
        "!kill -9 -1"
      ],
      "metadata": {
        "id": "-4O5fmsl4fDj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b1a2cea-01a3-4389-d160-51fedb6390c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------\n",
            "-- OPTIMIZATION START --\n",
            "------------------------\n",
            "\n",
            "Path (/content/gdrive/MyDrive/PhD/HiveNAS/baseline_threshold_3/) already exists!\n",
            "\n",
            "\n",
            "Would you like to overwrite files in this path? (y/n): y\n",
            "\n",
            "Found best-performing model {input|sep5x5_64|sep5x5_64|max_pool3x3|sep3x3_128|dropout|output} with a fitness score of 0.7039999961853027\n",
            "\n",
            "Epoch 1/50\n",
            "313/313 [==============================] - 99s 274ms/step - loss: 1.7103 - sparse_categorical_accuracy: 0.3823 - val_loss: 1.4811 - val_sparse_categorical_accuracy: 0.4621\n",
            "Epoch 2/50\n",
            "313/313 [==============================] - 85s 271ms/step - loss: 1.3720 - sparse_categorical_accuracy: 0.5094 - val_loss: 1.3335 - val_sparse_categorical_accuracy: 0.5254\n",
            "Epoch 3/50\n",
            "313/313 [==============================] - 85s 270ms/step - loss: 1.2244 - sparse_categorical_accuracy: 0.5673 - val_loss: 1.1701 - val_sparse_categorical_accuracy: 0.5886\n",
            "Epoch 4/50\n",
            "313/313 [==============================] - 85s 271ms/step - loss: 1.1233 - sparse_categorical_accuracy: 0.6026 - val_loss: 1.1170 - val_sparse_categorical_accuracy: 0.6012\n",
            "Epoch 5/50\n",
            "313/313 [==============================] - 84s 267ms/step - loss: 1.0476 - sparse_categorical_accuracy: 0.6326 - val_loss: 1.0390 - val_sparse_categorical_accuracy: 0.6408\n",
            "Epoch 6/50\n",
            "313/313 [==============================] - 83s 267ms/step - loss: 1.0013 - sparse_categorical_accuracy: 0.6475 - val_loss: 0.9867 - val_sparse_categorical_accuracy: 0.6466\n",
            "Epoch 7/50\n",
            "313/313 [==============================] - 83s 266ms/step - loss: 0.9528 - sparse_categorical_accuracy: 0.6639 - val_loss: 0.9209 - val_sparse_categorical_accuracy: 0.6749\n",
            "Epoch 8/50\n",
            "313/313 [==============================] - 84s 267ms/step - loss: 0.9122 - sparse_categorical_accuracy: 0.6787 - val_loss: 0.9006 - val_sparse_categorical_accuracy: 0.6846\n",
            "Epoch 9/50\n",
            "313/313 [==============================] - 84s 267ms/step - loss: 0.8998 - sparse_categorical_accuracy: 0.6852 - val_loss: 0.8868 - val_sparse_categorical_accuracy: 0.6873\n",
            "Epoch 10/50\n",
            "313/313 [==============================] - 83s 265ms/step - loss: 0.8568 - sparse_categorical_accuracy: 0.6988 - val_loss: 0.8674 - val_sparse_categorical_accuracy: 0.6991\n",
            "Epoch 11/50\n",
            "313/313 [==============================] - 83s 265ms/step - loss: 0.8359 - sparse_categorical_accuracy: 0.7080 - val_loss: 0.8404 - val_sparse_categorical_accuracy: 0.7087\n",
            "Epoch 12/50\n",
            "313/313 [==============================] - 83s 265ms/step - loss: 0.8134 - sparse_categorical_accuracy: 0.7142 - val_loss: 0.8332 - val_sparse_categorical_accuracy: 0.7025\n",
            "Epoch 13/50\n",
            "313/313 [==============================] - 83s 266ms/step - loss: 0.7993 - sparse_categorical_accuracy: 0.7198 - val_loss: 0.8340 - val_sparse_categorical_accuracy: 0.7040\n",
            "Epoch 14/50\n",
            "313/313 [==============================] - 83s 264ms/step - loss: 0.7810 - sparse_categorical_accuracy: 0.7264 - val_loss: 0.8149 - val_sparse_categorical_accuracy: 0.7179\n",
            "Epoch 15/50\n",
            "313/313 [==============================] - 82s 263ms/step - loss: 0.7671 - sparse_categorical_accuracy: 0.7312 - val_loss: 0.8043 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 16/50\n",
            "313/313 [==============================] - 83s 265ms/step - loss: 0.7469 - sparse_categorical_accuracy: 0.7378 - val_loss: 0.8069 - val_sparse_categorical_accuracy: 0.7130\n",
            "Epoch 17/50\n",
            "313/313 [==============================] - 83s 264ms/step - loss: 0.7336 - sparse_categorical_accuracy: 0.7437 - val_loss: 0.7824 - val_sparse_categorical_accuracy: 0.7249\n",
            "Epoch 18/50\n",
            "313/313 [==============================] - 82s 263ms/step - loss: 0.7211 - sparse_categorical_accuracy: 0.7478 - val_loss: 0.7840 - val_sparse_categorical_accuracy: 0.7243\n",
            "Epoch 19/50\n",
            "313/313 [==============================] - 82s 263ms/step - loss: 0.7137 - sparse_categorical_accuracy: 0.7520 - val_loss: 0.7622 - val_sparse_categorical_accuracy: 0.7369\n",
            "Epoch 20/50\n",
            "313/313 [==============================] - 83s 265ms/step - loss: 0.6963 - sparse_categorical_accuracy: 0.7563 - val_loss: 0.7487 - val_sparse_categorical_accuracy: 0.7393\n",
            "Epoch 21/50\n",
            "313/313 [==============================] - 82s 262ms/step - loss: 0.6889 - sparse_categorical_accuracy: 0.7582 - val_loss: 0.7549 - val_sparse_categorical_accuracy: 0.7404\n",
            "Epoch 22/50\n",
            "313/313 [==============================] - 82s 263ms/step - loss: 0.6845 - sparse_categorical_accuracy: 0.7600 - val_loss: 0.7449 - val_sparse_categorical_accuracy: 0.7416\n",
            "Epoch 23/50\n",
            "313/313 [==============================] - 82s 262ms/step - loss: 0.6783 - sparse_categorical_accuracy: 0.7636 - val_loss: 0.7859 - val_sparse_categorical_accuracy: 0.7307\n",
            "Epoch 24/50\n",
            "313/313 [==============================] - 83s 264ms/step - loss: 0.6671 - sparse_categorical_accuracy: 0.7673 - val_loss: 0.7302 - val_sparse_categorical_accuracy: 0.7455\n",
            "Epoch 25/50\n",
            "313/313 [==============================] - 82s 262ms/step - loss: 0.6507 - sparse_categorical_accuracy: 0.7698 - val_loss: 0.7605 - val_sparse_categorical_accuracy: 0.7350\n",
            "Epoch 26/50\n",
            "313/313 [==============================] - 82s 262ms/step - loss: 0.6449 - sparse_categorical_accuracy: 0.7731 - val_loss: 0.7218 - val_sparse_categorical_accuracy: 0.7509\n",
            "Epoch 27/50\n",
            "313/313 [==============================] - 82s 263ms/step - loss: 0.6332 - sparse_categorical_accuracy: 0.7775 - val_loss: 0.7261 - val_sparse_categorical_accuracy: 0.7447\n",
            "Epoch 28/50\n",
            "313/313 [==============================] - 83s 264ms/step - loss: 0.6340 - sparse_categorical_accuracy: 0.7781 - val_loss: 0.7534 - val_sparse_categorical_accuracy: 0.7415\n",
            "Epoch 29/50\n",
            "313/313 [==============================] - 82s 263ms/step - loss: 0.6244 - sparse_categorical_accuracy: 0.7793 - val_loss: 0.7302 - val_sparse_categorical_accuracy: 0.7484\n",
            "Epoch 30/50\n",
            "313/313 [==============================] - 83s 264ms/step - loss: 0.6161 - sparse_categorical_accuracy: 0.7820 - val_loss: 0.7163 - val_sparse_categorical_accuracy: 0.7515\n",
            "Epoch 31/50\n",
            "313/313 [==============================] - 83s 265ms/step - loss: 0.6193 - sparse_categorical_accuracy: 0.7836 - val_loss: 0.7108 - val_sparse_categorical_accuracy: 0.7584\n",
            "Epoch 32/50\n",
            "313/313 [==============================] - 83s 267ms/step - loss: 0.6002 - sparse_categorical_accuracy: 0.7886 - val_loss: 0.7165 - val_sparse_categorical_accuracy: 0.7538\n",
            "Epoch 33/50\n",
            "313/313 [==============================] - 83s 267ms/step - loss: 0.6009 - sparse_categorical_accuracy: 0.7903 - val_loss: 0.7062 - val_sparse_categorical_accuracy: 0.7558\n",
            "Epoch 34/50\n",
            "313/313 [==============================] - 83s 267ms/step - loss: 0.5943 - sparse_categorical_accuracy: 0.7928 - val_loss: 0.7472 - val_sparse_categorical_accuracy: 0.7469\n",
            "Epoch 35/50\n",
            "313/313 [==============================] - 84s 268ms/step - loss: 0.5822 - sparse_categorical_accuracy: 0.7958 - val_loss: 0.6972 - val_sparse_categorical_accuracy: 0.7590\n",
            "Epoch 36/50\n",
            "313/313 [==============================] - 83s 266ms/step - loss: 0.5802 - sparse_categorical_accuracy: 0.7960 - val_loss: 0.6831 - val_sparse_categorical_accuracy: 0.7645\n",
            "Epoch 37/50\n",
            "313/313 [==============================] - 84s 267ms/step - loss: 0.5797 - sparse_categorical_accuracy: 0.7954 - val_loss: 0.6872 - val_sparse_categorical_accuracy: 0.7593\n",
            "Epoch 38/50\n",
            "313/313 [==============================] - 84s 269ms/step - loss: 0.5755 - sparse_categorical_accuracy: 0.7974 - val_loss: 0.7212 - val_sparse_categorical_accuracy: 0.7502\n",
            "Epoch 39/50\n",
            "313/313 [==============================] - 84s 269ms/step - loss: 0.5655 - sparse_categorical_accuracy: 0.7998 - val_loss: 0.6938 - val_sparse_categorical_accuracy: 0.7612\n",
            "Epoch 40/50\n",
            "313/313 [==============================] - 84s 268ms/step - loss: 0.5593 - sparse_categorical_accuracy: 0.8027 - val_loss: 0.7430 - val_sparse_categorical_accuracy: 0.7516\n",
            "Epoch 41/50\n",
            "313/313 [==============================] - 84s 268ms/step - loss: 0.5513 - sparse_categorical_accuracy: 0.8069 - val_loss: 0.6914 - val_sparse_categorical_accuracy: 0.7621\n",
            "Epoch 42/50\n",
            "313/313 [==============================] - 84s 268ms/step - loss: 0.5504 - sparse_categorical_accuracy: 0.8062 - val_loss: 0.6907 - val_sparse_categorical_accuracy: 0.7664\n",
            "Epoch 43/50\n",
            "313/313 [==============================] - 83s 266ms/step - loss: 0.5571 - sparse_categorical_accuracy: 0.8051 - val_loss: 0.6760 - val_sparse_categorical_accuracy: 0.7665\n",
            "Epoch 44/50\n",
            "313/313 [==============================] - 83s 264ms/step - loss: 0.5482 - sparse_categorical_accuracy: 0.8078 - val_loss: 0.7001 - val_sparse_categorical_accuracy: 0.7622\n",
            "Epoch 45/50\n",
            "313/313 [==============================] - 82s 264ms/step - loss: 0.5481 - sparse_categorical_accuracy: 0.8073 - val_loss: 0.6880 - val_sparse_categorical_accuracy: 0.7666\n",
            "Epoch 46/50\n",
            "313/313 [==============================] - 83s 265ms/step - loss: 0.5479 - sparse_categorical_accuracy: 0.8076 - val_loss: 0.6795 - val_sparse_categorical_accuracy: 0.7652\n",
            "Epoch 47/50\n",
            "313/313 [==============================] - 82s 262ms/step - loss: 0.5357 - sparse_categorical_accuracy: 0.8111 - val_loss: 0.6846 - val_sparse_categorical_accuracy: 0.7615\n",
            "Epoch 48/50\n",
            "313/313 [==============================] - 82s 264ms/step - loss: 0.5312 - sparse_categorical_accuracy: 0.8127 - val_loss: 0.7210 - val_sparse_categorical_accuracy: 0.7510\n",
            "Epoch 49/50\n",
            "313/313 [==============================] - 83s 266ms/step - loss: 0.5231 - sparse_categorical_accuracy: 0.8171 - val_loss: 0.6764 - val_sparse_categorical_accuracy: 0.7704\n",
            "Epoch 50/50\n",
            "313/313 [==============================] - 84s 267ms/step - loss: 0.5191 - sparse_categorical_accuracy: 0.8176 - val_loss: 0.6862 - val_sparse_categorical_accuracy: 0.7690\n",
            "79/79 [==============================] - 1s 13ms/step - loss: 0.6446 - sparse_categorical_accuracy: 0.7863\n",
            "---------------------\n",
            "-- OPTIMIZATION END --\n",
            " === TOTAL TIME TAKEN: 4245.8657422065735 ==== \n",
            "---------------------\n",
            "{'fitness': 0.786300003528595, 'epochs': 50, 'filename': '3ab2efe1627b9be99a1e30c7443dc1ed8176c56f', 'params': 118514282}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Debug Utils"
      ],
      "metadata": {
        "id": "tjXeDtjA8V6m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPqR0qTviziy"
      },
      "outputs": [],
      "source": [
        "def delete_all():\n",
        "    '''\n",
        "        ***DESTRUCTIVE ACTION***\n",
        "        Removes all weight files and results CSV.\n",
        "        Used for freeing up space from faulty runs.\n",
        "\n",
        "        --Make sure the file paths are entered correctly in Params--\n",
        "    '''\n",
        "    for root, dirs, files in os.walk(Params.get_results_path() + Params['WEIGHT_FILES_SUBPATH']):\n",
        "        for f in files:\n",
        "            filepath = os.path.join(Params.get_results_path() + Params['WEIGHT_FILES_SUBPATH'], f)\n",
        "            os.remove(filepath)\n",
        "\n",
        "    os.remove(os.path.join(Params.get_results_path(), f'{Params[\"CONFIG_VERSION\"]}.csv'))\n",
        "\n",
        "# delete_all()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' Test ImageAugmentation (before/after plot) '''\n",
        "\n",
        "gc.collect()\n",
        "image_augmentor = ImageDataGenerator(\n",
        "                                    zoom_range = [0.8, 1.1], \n",
        "                                    shear_range= 10,\n",
        "                                    rotation_range=15,\n",
        "                                    width_shift_range=0.1,\n",
        "                                    height_shift_range=0.1,\n",
        "                                    horizontal_flip=True,\n",
        "                                    preprocessing_function=ImgAug.augment,\n",
        "                                    validation_split=0.2)\n",
        "image_augmentor_before = ImageDataGenerator()\n",
        "\n",
        "\n",
        "import requests\n",
        "DownURL = \"https://images.pexels.com/photos/1056251/pexels-photo-1056251.jpeg?crop=entropy&cs=srgb&dl=pexels-ihsan-aditya-1056251.jpg&fit=crop&fm=jpg&h=426&w=640\"\n",
        "img_data = requests.get(DownURL).content\n",
        "\n",
        "FileHandler.create_dir('/content/sample_data/impath/cat')\n",
        "\n",
        "with open('/content/sample_data/impath/cat/cat-small.jpg', 'wb') as handler:\n",
        "    handler.write(img_data)\n",
        "\n",
        "data_before = image_augmentor_before.flow_from_directory(\n",
        "    \"/content/sample_data/impath\",\n",
        "    target_size=(213, 320),\n",
        "    batch_size=1,\n",
        ")\n",
        "data = image_augmentor.flow_from_directory(\n",
        "    \"/content/sample_data/impath\",\n",
        "    target_size=(213, 320),\n",
        "    batch_size=1,\n",
        ")\n",
        "\n",
        "fig, axs = plt.subplots(1,2, figsize=(15,12))\n",
        "\n",
        "plt.subplot(axs[0])\n",
        "plt.imshow(data_before.next()[0][0].astype('int'))\n",
        "plt.title(\"Before\")\n",
        "\n",
        "plt.subplot(axs[1])\n",
        "plt.imshow(data.next()[0][0].astype('int'))\n",
        "plt.title(\"After\")"
      ],
      "metadata": {
        "id": "GTGYU9-hzMkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Unit Testing '''\n",
        "\n",
        "Logger.EVALUATION_LOGGING = False\n",
        "\n",
        "objective_interface = Sphere(10)\n",
        "\n",
        "abc = ArtificialBeeColony(objective_interface)\n",
        "\n",
        "abc.optimize()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nZwgEc431p49",
        "outputId": "5462c7b7-f3fa-46c9-b7df-2b37722237a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " [22547.432820331276, 22377.235423088005, 34264.45928373387]\n",
            "[0.2847307368096651, 0.28258156614639535, 0.43268769704393956] \n",
            "\n",
            "0\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "STATUS: itr: 0 -- Best fitness: 64579.76415445288, Total time (s): 0.27864909172058105\n",
            "\n",
            "\n",
            " [30471.270200006005, 33301.36720015647, 64514.92356032946]\n",
            "[0.23752542643718094, 0.2595854827574742, 0.5028890908053448] \n",
            "\n",
            "1\n",
            "2\n",
            "2\n",
            "1\n",
            "1\n",
            "2\n",
            "2\n",
            "STATUS: itr: 1 -- Best fitness: 64899.020982095564, Total time (s): 0.7251143455505371\n",
            "\n",
            "\n",
            " [45139.54111111039, 48068.55320695488, 37030.11248240911]\n",
            "[0.3465918523026614, 0.36908098740666606, 0.2843271602906725] \n",
            "\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "2\n",
            "1\n",
            "1\n",
            "STATUS: itr: 2 -- Best fitness: 78222.27769850551, Total time (s): 1.1666624546051025\n",
            "\n",
            "\n",
            " [23477.709085288727, 35239.14909191979, 44833.063627461364]\n",
            "[0.22673149802162237, 0.34031052410469576, 0.4329579778736818] \n",
            "\n",
            "2\n",
            "2\n",
            "1\n",
            "1\n",
            "2\n",
            "2\n",
            "1\n",
            "STATUS: itr: 3 -- Best fitness: 78222.27769850551, Total time (s): 1.981539249420166\n",
            "\n",
            "\n",
            " [46989.083375049115, 35239.14909191979, 48552.579073174464]\n",
            "[0.3592958701974041, 0.2694534489928291, 0.3712506808097668] \n",
            "\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "0\n",
            "0\n",
            "1\n",
            "STATUS: itr: 4 -- Best fitness: 78222.27769850551, Total time (s): 2.7483623027801514\n",
            "\n",
            "\n",
            " [56487.797206542455, 33497.943642927225, 53046.60580199251]\n",
            "[0.3949289356021216, 0.23420045762922462, 0.37087060676865374] \n",
            "\n",
            "1\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "1\n",
            "2\n",
            "STATUS: itr: 5 -- Best fitness: 78222.27769850551, Total time (s): 3.403900623321533\n",
            "\n",
            "\n",
            " [24257.077549582635, 51834.47795331884, 73042.66469035394]\n",
            "[0.16265609294680738, 0.3475690232535462, 0.4897748837996463] \n",
            "\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "1\n",
            "2\n",
            "2\n",
            "STATUS: itr: 6 -- Best fitness: 81284.9828792448, Total time (s): 4.144596576690674\n",
            "\n",
            "\n",
            " [24257.077549582635, 49933.897601966855, 36903.38825949406]\n",
            "[0.21834971420367008, 0.4494696909882121, 0.33218059480811774] \n",
            "\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-db63b5023a21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mabc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArtificialBeeColony\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_interface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-60-07ea6e2ddf25>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mitr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATIONS_COUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__employee_bee_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__onlooker_bee_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__update_solutions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__scout_bee_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-07ea6e2ddf25>\u001b[0m in \u001b[0;36m__onlooker_bee_phase\u001b[0;34m(self, itr)\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;31m# Evaluate employee position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monlooker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj_interface\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__save_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0;31m# Already evaluated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-07ea6e2ddf25>\u001b[0m in \u001b[0;36m__save_results\u001b[0;34m(self, series)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;31m### Make sure folders are created on Drive/local system before running\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRESULTS_FILE_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mParams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRESULTS_FILE_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mLogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilesave_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'candidate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weights_filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3480\u001b[0m             \u001b[0mdoublequote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3481\u001b[0m             \u001b[0mescapechar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mescapechar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3482\u001b[0;31m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3483\u001b[0m         )\n\u001b[1;32m   3484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1103\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         )\n\u001b[0;32m-> 1105\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m             )\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreated_handles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                 \u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bSvzxJdA2g6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GZwutf5s9vN3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7bd294f16cc648bebd26bd1461b19ebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f3bf9039a3246c8aeb20811b1d5a779",
              "IPY_MODEL_d939d647d5b644dc91211aea39fd4242"
            ],
            "layout": "IPY_MODEL_13ffdd83aca64f7a8a6530f3a6689cf7"
          }
        },
        "3f3bf9039a3246c8aeb20811b1d5a779": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_b1c84884bbeb4850b4a281bb3d76dd5a",
            "placeholder": "/path/to/config.yaml",
            "style": "IPY_MODEL_94d00389d9b24553839304d468a355a3",
            "value": "/content/hivenas_aug_a.yaml"
          }
        },
        "d939d647d5b644dc91211aea39fd4242": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d38c2610c1b14d148f0e1335c2b08657",
              "IPY_MODEL_50a6fdb28783486c94bfde3e55c70d8a"
            ],
            "layout": "IPY_MODEL_91221d08fb2e421ca2e1182f71e49c7c"
          }
        },
        "13ffdd83aca64f7a8a6530f3a6689cf7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1c84884bbeb4850b4a281bb3d76dd5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94d00389d9b24553839304d468a355a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d38c2610c1b14d148f0e1335c2b08657": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Load Config",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_2a04366eac8f4d178792418cac6efe59",
            "style": "IPY_MODEL_17aa6bf895d64bc78e7b974f85bdd7ab",
            "tooltip": ""
          }
        },
        "50a6fdb28783486c94bfde3e55c70d8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Export *Form* Config",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_23e17a91caf649a2a4ad5c14b3849863",
            "style": "IPY_MODEL_5e3ad6ffab204e839c56c6f520f41872",
            "tooltip": ""
          }
        },
        "91221d08fb2e421ca2e1182f71e49c7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a04366eac8f4d178792418cac6efe59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17aa6bf895d64bc78e7b974f85bdd7ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "23e17a91caf649a2a4ad5c14b3849863": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e3ad6ffab204e839c56c6f520f41872": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "3896eca9fc86436b913d714df2e90e16": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_4f8efa4db817469b83dc9ed000360666",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "\n",
                  "Successfully loaded the operational parameters from /content/hivenas_aug_a.yaml.\n",
                  "\n",
                  "\n"
                ]
              }
            ]
          }
        },
        "4f8efa4db817469b83dc9ed000360666": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}